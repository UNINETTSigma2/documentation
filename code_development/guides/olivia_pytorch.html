

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PyTorch on Olivia &mdash; Sigma2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/nris.css?v=69e7a171" />
      <link rel="stylesheet" type="text/css" href="../../_static/universal-navbar.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/statuspal.css" />

  
    <link rel="shortcut icon" href="../../_static/nris.ico"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://siteimproveanalytics.com/js/siteanalyze_6036825.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/statuspal_widget.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Profiling a Resnet-18 model on Multiple GPUs" href="PyTorchProfilerMultiGpu.html" /> 
</head>

<body class="wy-body-for-nav">
<!-- Send url to parent when displayed as iframe -->
<script>
    const valid_orign_url = "https://www.sigma2.no"
    window.addEventListener('message', function(event) {
        if (event.data === 'getDocumentationIframeUrl' && event.origin.startsWith(valid_orign_url)) {
            // path only (/path/example.html)
            const path = window.location.pathname
            // query string (including the initial ? symbol)
            const search = window.location.search
            // Returns the hash (including the initial # symbol)
            const hash = window.location.hash
            const newUrl = path + search + hash;
            event.source.postMessage(newUrl, event.origin)
        }
    })

</script>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Sigma2/NRIS documentation
              <img src="../../_static/NRIS Logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Policies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../code-of-conduct.html">Code of Conduct</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/support_line.html">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/extended_support.html">Extended support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/how_to_write_good_support_requests.html">Writing good support requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/qa-sessions.html">Open Question &amp; Answer Sessions for All Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/lost_forgotten_password.html">Lost, expiring or changing passwords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/two_factor_authentication.html">One-time-pad (OTP) / Two-factor authentication</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/project-leader-handbook">Project Leader Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/events.html">Training events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/notes_qa.html">Questions, Answers and Feedbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/past_training.html">An overview over training events in the past</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/videos.html">Training Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/short_instructions.html">Short Instructions Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/material.html">Training materials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/opslog.html">Status and maintenance of systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/applying_account.html">How do I get an account?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/applying_resources.html">Applying for computing and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/file_transfer.html">File transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/editing_files.html">Editing files</a></li>
<li class="toctree-l1"><a class="reference internal" href="vs_code/connect_to_server.html">Connecting to a system with Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ssh.html">SSH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ssh.html#common-ssh-errors">Common SSH errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ood.html">Open OnDemand</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/R.html">First R calculation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Files, storage and backup</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird_lmd.html">NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/clusters.html">Storage areas on HPC clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/quota.html">Storage quota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/backup.html">Backup on Betzy, Fram, Saga, and NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/sharing_files.html">Data handling and storage policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/performance.html">Optimizing storage performance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HPC usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/migration2metacenter.html">Migration to an NRIS HPC machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing/responsible-use.html">Using shared resources responsibly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/overview.html">Running jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/internet-login-compute-nodes.html">Login nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/internet-login-compute-nodes.html#compute-nodes">Compute nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing/tuning-applications.html">Tuning applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides_llm.html">Running LLM Models in a Cluster Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compute resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/hardware_overview.html">Overview over our machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/betzy.html">Betzy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/fram.html">Fram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/olivia.html">Olivia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/saga.html">Saga</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/lumi.html">LUMI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../software/modulescheme.html">Software module scheme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/installed_software.html">Installed software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/userinstallsw.html">Installing software as user</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/appguides.html">Application guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/licenses.html">Licence and access policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/eessi.html">EESSI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../nird_archive/sandbox-user-guide.html">NIRD Research Data Archive Sandbox (NIRD RDA sandbox)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_archive/user-guide.html">NIRD Research Data Archive (NIRD RDA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_toolkit/overview.html">NIRD Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_service_platform/overview_nird_service_platform.html">NIRD Service Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../services/easydmp-user-documentation.html">EasyDMP User Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/course_resources.html">CRaaS - Course Resources as a Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code development and tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../overview.html">Code development and tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../overview.html#id1">Code development</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../overview.html#tutorials">Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../guides_gpu.html">GPU programming models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_ml.html">Machine Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_containers_gpu.html">Containers with GPU support</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_monitor_gpu.html">Monitoring GPU accelerated applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_python.html">Python libraries</a></li>
<li class="toctree-l3"><a class="reference internal" href="PyTorchProfilerMultiGpu.html">Profiling a Resnet-18 model on Multiple GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="PyTorchProfilerMultiGpu.html#performance-metrics">Performance Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="PyTorchProfilerMultiGpu.html#conclusion">Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="PyTorchProfilerMultiGpu.html#relevant-links">Relevant links</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">PyTorch on Olivia</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#key-considerations">Key Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-a-resnet-model-with-the-cifar-100-dataset">Training a ResNet Model with the CIFAR-100 Dataset</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Sigma2/NRIS documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../overview.html">Code development and tutorials</a></li>
      <li class="breadcrumb-item active">PyTorch on Olivia</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pytorch-on-olivia">
<span id="pytorch-olivia"></span><h1><a class="toc-backref" href="#id1" role="doc-backlink">PyTorch on Olivia</a><a class="headerlink" href="#pytorch-on-olivia" title="Link to this heading"></a></h1>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#pytorch-on-olivia" id="id1">PyTorch on Olivia</a></p>
<ul>
<li><p><a class="reference internal" href="#key-considerations" id="id2">Key Considerations</a></p></li>
<li><p><a class="reference internal" href="#training-a-resnet-model-with-the-cifar-100-dataset" id="id3">Training a ResNet Model with the CIFAR-100 Dataset</a></p>
<ul>
<li><p><a class="reference internal" href="#single-gpu-implementation" id="id4">Single GPU Implementation</a></p></li>
<li><p><a class="reference internal" href="#multi-gpu-implementation" id="id5">Multi-GPU Implementation</a></p></li>
<li><p><a class="reference internal" href="#multi-node-setup" id="id6">Multi-Node Setup</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<p>In this guide, we will test PyTorch on the Olivia system, which uses the
Aarch64 architecture on its compute nodes. We use Nvidia’s PyTorch container.
Training a ResNet model in a containerized environment bypasses the need to
manually download and install PyTorch wheels. The container includes all the
necessary packages to run the project, simplifying the setup process.</p>
<section id="key-considerations">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Key Considerations</a><a class="headerlink" href="#key-considerations" title="Link to this heading"></a></h2>
<ol class="arabic">
<li><p><strong>Different Architectures</strong>:</p>
<p>The login node and the compute node on Olivia have different architectures.
The login node uses the x86_64 architecture, while the compute node uses
Aarch64. This means we cannot install software directly on the login node and
expect it to work on the compute node.</p>
</li>
<li><p><strong>CUDA Version</strong>:</p>
<p>The compute nodes are equipped with CUDA version 12.7, as confirmed by running
the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command. Therefore, we must ensure that the container we use is
compatible with this CUDA version.</p>
</li>
</ol>
</section>
<section id="training-a-resnet-model-with-the-cifar-100-dataset">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Training a ResNet Model with the CIFAR-100 Dataset</a><a class="headerlink" href="#training-a-resnet-model-with-the-cifar-100-dataset" title="Link to this heading"></a></h2>
<p>To test Olivia’s capabilities with real-world workloads, we will train a ResNet
model using the CIFAR-100 dataset under the following scenarios:</p>
<ol class="arabic simple">
<li><p>Single GPU</p></li>
<li><p>Multiple GPUs</p></li>
<li><p>Multiple Nodes</p></li>
</ol>
<p>The primary goal of this exercise is to verify that we can successfully run
training tasks on Olivia. As such, we will not delve into the specifics of
neural network training in this documentation. A separate guide will be prepared
to cover those details.</p>
<section id="single-gpu-implementation">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Single GPU Implementation</a><a class="headerlink" href="#single-gpu-implementation" title="Link to this heading"></a></h3>
<p>To train the ResNet model on a single GPU, we use the following files, which
include the main Python script responsible for training the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># resnet.py</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This script trains a WideResNet model on the CIFAR-100  dataset without using Distributed Data Parallel (DDP).</span>
<span class="sd">The goal is to provide a simple, single-GPU implementation of the training process.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="c1"># Import custom modules</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">data.dataset_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_cifar100</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">models.wide_resnet</span><span class="w"> </span><span class="kn">import</span> <span class="n">WideResNet</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">training.train_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">utils.device_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_device</span>

<span class="c1"># Define paths</span>
<span class="n">shared_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">),</span> <span class="s2">&quot;../shared&quot;</span><span class="p">))</span>
<span class="n">images_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">shared_dir</span><span class="p">,</span> <span class="s2">&quot;images&quot;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">images_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Hyperparameters</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">TARGET_ACCURACY</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">PATIENCE</span> <span class="o">=</span> <span class="mi">2</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train_resnet_without_ddp</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains a WideResNet model on the CIFAR-100 dataset without DDP.</span>
<span class="sd">    Args:</span>
<span class="sd">        batch_size (int): Batch size for training.</span>
<span class="sd">        epochs (int): Number of epochs to train.</span>
<span class="sd">        learning_rate (float): Learning rate for the optimizer.</span>
<span class="sd">        device (torch.device): Device to run training on (CPU or GPU).</span>
<span class="sd">    Returns:</span>
<span class="sd">        throughput (float): Images processed per second</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training WideResNet on CIFAR-100 with Batch Size: </span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Training variables</span>
    <span class="n">val_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_images</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># total images processed</span>
    <span class="c1"># Load the dataset</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_cifar100</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="c1"># Initialize the WideResNet Model</span>
    <span class="c1"># For fashion MNIST dataset</span>
    <span class="c1">#num_classes = 10</span>

    <span class="c1"># For CIFAR-100 dataset</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">WideResNet</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Define the loss function and optimizer</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c1"># Train the model for one epoch</span>
        <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="c1"># Calculate epoch time</span>
        <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
        <span class="n">total_time</span> <span class="o">+=</span> <span class="n">epoch_time</span>
        <span class="c1"># Compute throughput (images per second)</span>
        <span class="n">images_per_sec</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">/</span> <span class="n">epoch_time</span>
        <span class="n">total_images</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">batch_size</span>
        <span class="c1"># Compute validation accuracy and loss</span>
        <span class="n">v_accuracy</span><span class="p">,</span> <span class="n">v_loss</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="n">val_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v_accuracy</span><span class="p">)</span>
        <span class="c1"># Print metrics</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Epoch = </span><span class="si">{:2d}</span><span class="s2">: Epoch Time = </span><span class="si">{:5.3f}</span><span class="s2">, Validation Loss = </span><span class="si">{:5.3f}</span><span class="s2">, Validation Accuracy = </span><span class="si">{:5.3f}</span><span class="s2">, Images/sec = </span><span class="si">{:5.3f}</span><span class="s2">, Cumulative Time = </span><span class="si">{:5.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">epoch_time</span><span class="p">,</span> <span class="n">v_loss</span><span class="p">,</span> <span class="n">v_accuracy</span><span class="p">,</span> <span class="n">images_per_sec</span><span class="p">,</span> <span class="n">total_time</span>
        <span class="p">))</span>
        <span class="c1"># Early stopping</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">PATIENCE</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">acc</span> <span class="o">&gt;=</span> <span class="n">TARGET_ACCURACY</span> <span class="k">for</span> <span class="n">acc</span> <span class="ow">in</span> <span class="n">val_accuracy</span><span class="p">[</span><span class="o">-</span><span class="n">PATIENCE</span><span class="p">:]):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Early stopping after epoch </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">break</span>
    <span class="c1"># Final metrics</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">total_images</span> <span class="o">/</span> <span class="n">total_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training complete. Final Validation Accuracy = </span><span class="si">{:5.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total Training Time: </span><span class="si">{:5.3f}</span><span class="s2"> seconds&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">total_time</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Throughput: </span><span class="si">{:5.3f}</span><span class="s2"> images/second&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">throughput</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">throughput</span>
<span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Set the compute device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
    <span class="c1"># Train the WideResNet model</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">train_resnet_without_ddp</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Single-GPU Throughput: </span><span class="si">{</span><span class="n">throughput</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> images/second&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>This file contains the data utility functions used for preparing and managing
the dataset. Please note that the CIFAR-100 dataset will be installed and placed in the
respective folder by the job script.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset_utils.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">load_cifar100</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Loads the CIFAR-100 dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Define transformations</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5071</span><span class="p">,</span> <span class="mf">0.4867</span><span class="p">,</span> <span class="mf">0.4408</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2675</span><span class="p">,</span> <span class="mf">0.2565</span><span class="p">,</span> <span class="mf">0.2761</span><span class="p">))</span> <span class="c1"># CIFAR-100 mean and std</span>
    <span class="p">])</span>
    <span class="c1"># Load full datasets</span>
    <span class="n">train_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR100</span><span class="p">(</span>
            <span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/shared/data/&quot;</span><span class="p">,</span><span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">test_set</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR100</span><span class="p">(</span><span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/shared/data/&quot;</span><span class="p">,</span><span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="c1"># Create the data loaders</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="n">sampler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span><span class="n">sampler</span><span class="o">=</span> <span class="n">sampler</span> <span class="p">,</span><span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span><span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span><span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span>
</pre></div>
</div>
<p>This file includes the device utility functions, which handle device selection
and management for training (e.g., selecting the appropriate GPU).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># device_utils.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_device</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Determine the compute device (GPU or CPU).</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.device: The device to use for the computations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>This file contains the implementation of the ResNet model architecture.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># wide_resnet.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Standard convolution block followed by batch normalization</span>
<span class="k">class</span><span class="w"> </span><span class="nc">cbrblock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">cbrblock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cbr</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cbr</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Basic residual block</span>
<span class="k">class</span><span class="w"> </span><span class="nc">conv_block</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">scale_input</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">conv_block</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_input</span> <span class="o">=</span> <span class="n">scale_input</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_input</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span> <span class="o">=</span> <span class="n">cbrblock</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span> <span class="o">=</span> <span class="n">cbrblock</span><span class="p">(</span><span class="n">output_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_input</span><span class="p">:</span>
            <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">residual</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span> <span class="o">+</span> <span class="n">residual</span>

<span class="c1"># WideResnet model</span>
<span class="k">class</span><span class="w"> </span><span class="nc">WideResNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WideResNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># RGB images (3 channels) input for CIFAR-100 dataset</span>
        <span class="n">nChannels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">320</span><span class="p">,</span> <span class="mi">640</span><span class="p">]</span>
        <span class="c1"># Grayscale images (1 channel) for Fashion MNIST dataset</span>
        <span class="c1"># nChannels = [1, 16, 160, 320, 640]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_block</span> <span class="o">=</span> <span class="n">cbrblock</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">scale_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block2</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">scale_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block3</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">scale_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block4</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">scale_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block5</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">scale_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block6</span> <span class="o">=</span> <span class="n">conv_block</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">nChannels</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">scale_input</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># Global Average pooling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pool</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
        <span class="c1"># Fully connected layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flat</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">nChannels</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_block</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block4</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block5</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">block6</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flat</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

</pre></div>
</div>
<p>Finally, this file serves as a utility module for importing the training and
testing datasets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># train_utils.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains the model for one epoch. Note, that we will use this only for single GPU implementation.</span>
<span class="sd">    Args:</span>
<span class="sd">        model(torch.nn.Module): The model to train.</span>
<span class="sd">        optimizer(torch.optim.Optimizer): Optimizer for updating model parameters.</span>
<span class="sd">        train_loader(torch.utils.data.DataLoader): DataLoader for training data.</span>
<span class="sd">        loss_fn (torch.nn.Module): Loss function.</span>
<span class="sd">        device (torch.device): Device to run training on (CPU or GPU).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># Forward passs</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="c1"># Backward pass and optimization</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>



<span class="k">def</span><span class="w"> </span><span class="nf">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluates the model on the validation dataset.</span>
<span class="sd">    Args:</span>
<span class="sd">        model(torch.nn.Module): The model to evaluate.</span>
<span class="sd">        test_loader (torch.utils.data.DataLoader): DataLoader for validation data.</span>
<span class="sd">        loss_fn (torch.nn.Module): Loss function.</span>
<span class="sd">        device (torch.device): Device to run evaluation on (CPU or GPU).</span>
<span class="sd">    Returns:</span>
<span class="sd">        tuple: Validation accuracy and validation loss.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">total_labels</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">correct_labels</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">loss_total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Forward pass</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="c1"># Compute accuracy and loss</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">total_labels</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="n">correct_labels</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">loss_total</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">v_accuracy</span> <span class="o">=</span> <span class="n">correct_labels</span> <span class="o">/</span> <span class="n">total_labels</span>
    <span class="n">v_loss</span> <span class="o">=</span> <span class="n">loss_total</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">v_accuracy</span><span class="p">,</span> <span class="n">v_loss</span>
</pre></div>
</div>
<p>Since we are using the container, to provide the container access to additional
directories, such as the one containing the training scripts, we must explicitly
bind these directories using the <code class="docutils literal notranslate"><span class="pre">--bind</span></code> option.</p>
<p>The job scripts below shows how it is done for accessing the required files
inside the container.</p>
<p>Note that the command to run the script includes the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> option, which ensures
that the container has access to GPU resources. This is essential for leveraging
hardware acceleration during training.</p>
<section id="job-script-for-single-gpu-training">
<h4>Job Script for Single GPU Training<a class="headerlink" href="#job-script-for-single-gpu-training" title="Link to this heading"></a></h4>
<p>In the job script below, we will use a single GPU for training. The container is
downloaded and placed at <code class="docutils literal notranslate"><span class="pre">/cluster/work/support/container/pytorch_nvidia_25.05_arm64.sif</span></code>.
This container contains all the necessary packages for running our deep learning
training (e.g., torch and torchvision). Moreover, we bind the host directory
within the container using the <code class="docutils literal notranslate"><span class="pre">--bind</span></code> option and export the PYTHONPATH to include
the shared directory for module imports. Finally, we run the <code class="docutils literal notranslate"><span class="pre">apptainer</span> <span class="pre">exec</span> <span class="pre">--nv</span></code>
command, binding the host directory and using torchrun.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=simple_nn_training</span>
<span class="c1">#SBATCH --account=&lt;project_number&gt;</span>
<span class="c1">#SBATCH --time=00:10:00</span>
<span class="c1">#SBATCH --output=resnet_with_container_%j.out</span>
<span class="c1">#SBATCH --error=resnet_with_container_%j.err</span>
<span class="c1">#SBATCH --partition=accel</span>
<span class="c1">#SBATCH --nodes=1                    # Single compute node</span>
<span class="c1">#SBATCH --ntasks-per-node=1          # One task (process) on the node</span>
<span class="c1">#SBATCH --cpus-per-task=72           # Reserve 72 CPU cores</span>
<span class="c1">#SBATCH --mem-per-gpu=110G           # Request 110 GB of CPU RAM per GPU</span>
<span class="c1">#SBATCH --gpus-per-node=1            # Request 1 GPU</span>

<span class="c1"># Path to the container</span>
<span class="nv">CONTAINER_PATH</span><span class="o">=</span><span class="s2">&quot;/cluster/work/support/container/pytorch_nvidia_25.05_arm64.sif&quot;</span>

<span class="c1"># Path to the training script</span>
<span class="nv">TRAINING_SCRIPT</span><span class="o">=</span><span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/simple_nn_project/resnet.py&quot;</span>

<span class="c1"># Bind the directory</span>
<span class="nv">BIND_DIR</span><span class="o">=</span><span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private&quot;</span>


<span class="c1"># Set PYTHONPATH to include the shared directory</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/shared:<span class="nv">$PYTHONPATH</span>

<span class="c1"># Check GPU availability inside the container</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Checking GPU availability inside the container...&quot;</span>
apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>--bind<span class="w"> </span><span class="nv">$BIND_DIR</span><span class="w"> </span><span class="nv">$CONTAINER_PATH</span><span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())&#39;</span>

<span class="c1"># Start GPU utilization monitoring in the background</span>
<span class="nv">GPU_LOG_FILE</span><span class="o">=</span><span class="s2">&quot;gpu_utilization_resnet_with_container.log&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Starting GPU utilization monitoring...&quot;</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>timestamp,index,name,utilization.gpu,utilization.memory,memory.total,memory.used<span class="w"> </span>--format<span class="o">=</span>csv<span class="w"> </span>-l<span class="w"> </span><span class="m">5</span><span class="w"> </span>&gt;<span class="w"> </span><span class="nv">$GPU_LOG_FILE</span><span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Run the training script with torchrun inside the container</span>
apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>--bind<span class="w"> </span><span class="nv">$BIND_DIR</span><span class="w"> </span><span class="nv">$CONTAINER_PATH</span><span class="w"> </span><span class="se">\t</span>orchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="nv">$SLURM_JOB_NUM_NODES</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="nv">$SLURM_GPUS_ON_NODE</span><span class="w"> </span><span class="nv">$TRAINING_SCRIPT</span>


<span class="c1"># Stop GPU utilization monitoring</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Stopping GPU utilization monitoring...&quot;</span>
pkill<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;nvidia-smi --query-gpu&quot;</span>
</pre></div>
</div>
<p>Output of the training is shown below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">Epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">95</span>:<span class="w"> </span>Epoch<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span>.235,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.331,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.739,<span class="w"> </span>Images/sec<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2470</span>.132,<span class="w"> </span>Cumulative<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1893</span>.528
<span class="nv">Epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">96</span>:<span class="w"> </span>Epoch<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span>.331,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.313,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.743,<span class="w"> </span>Images/sec<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2458</span>.565,<span class="w"> </span>Cumulative<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1913</span>.859
<span class="nv">Epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">97</span>:<span class="w"> </span>Epoch<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span>.173,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.325,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.742,<span class="w"> </span>Images/sec<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2477</span>.718,<span class="w"> </span>Cumulative<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1934</span>.032
<span class="nv">Epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">98</span>:<span class="w"> </span>Epoch<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span>.168,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.328,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.736,<span class="w"> </span>Images/sec<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2478</span>.441,<span class="w"> </span>Cumulative<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1954</span>.200
<span class="nv">Epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">99</span>:<span class="w"> </span>Epoch<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span>.065,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.323,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.739,<span class="w"> </span>Images/sec<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2491</span>.122,<span class="w"> </span>Cumulative<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1974</span>.265
<span class="nv">Epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span>:<span class="w"> </span>Epoch<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">19</span>.773,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.332,<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.739,<span class="w"> </span>Images/sec<span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2527</span>.948,<span class="w"> </span>Cumulative<span class="w"> </span><span class="nv">Time</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1994</span>.037

Training<span class="w"> </span>complete.<span class="w"> </span>Final<span class="w"> </span>Validation<span class="w"> </span><span class="nv">Accuracy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span>.739
Total<span class="w"> </span>Training<span class="w"> </span>Time:<span class="w"> </span><span class="m">1994</span>.037<span class="w"> </span>seconds
Throughput:<span class="w"> </span><span class="m">2506</span>.673<span class="w"> </span>images/second
Single-GPU<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">2506</span>.673<span class="w"> </span>images/second
</pre></div>
</div>
<p>The output suggests that the total throughput obtained from single GPU training
is <code class="docutils literal notranslate"><span class="pre">2506.673</span> <span class="pre">images/second</span></code> and it took approximately <code class="docutils literal notranslate"><span class="pre">1994.037</span> <span class="pre">seconds</span></code> to
complete the training. As we move forward with the multi-GPU implementation, our
goal is to achieve higher throughput and reduced training time.</p>
</section>
</section>
<section id="multi-gpu-implementation">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Multi-GPU Implementation</a><a class="headerlink" href="#multi-gpu-implementation" title="Link to this heading"></a></h3>
<p>To scale our training to multiple GPUs, we will utilize PyTorch’s Distributed
Data Parallel (DDP) framework. DDP allows us to efficiently scale training
across multiple GPUs and even across multiple nodes. To learn more about how to
implement DDP, please refer to this official documentation from PyTorch
<a class="reference external" href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html">Getting Started with DDP</a></p>
<p>For this, we need to modify the main Python script to include DDP
implementation. The updated script will work for both scenarios: Multiple GPUs
within a single node and Multiple nodes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># singlenode.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">argparse</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_process_group</span><span class="p">,</span> <span class="n">destroy_process_group</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">data.dataset_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_cifar100</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">models.wide_resnet</span><span class="w"> </span><span class="kn">import</span> <span class="n">WideResNet</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">training.train_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">test</span>
<span class="c1"># Parse input arguments</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;CIFAR-100 DDP example with Mixed Precision&#39;</span><span class="p">,</span>
                                 <span class="n">formatter_class</span><span class="o">=</span><span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentDefaultsHelpFormatter</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--batch-size&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Input batch size for training&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--epochs&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Number of epochs to train&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--base-lr&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Learning rate for single GPU&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--target-accuracy&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">0.85</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Target accuracy to stop training&#39;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--patience&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Number of epochs that meet target before stopping&#39;</span><span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
<span class="k">def</span><span class="w"> </span><span class="nf">ddp_setup</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Set up the distributed environment.&quot;&quot;&quot;</span>
    <span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">]))</span>
<span class="k">def</span><span class="w"> </span><span class="nf">main_worker</span><span class="p">():</span>
    <span class="n">ddp_setup</span><span class="p">()</span>
    <span class="c1"># Get the local rank and device</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">])</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RANK&quot;</span><span class="p">])</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WORLD_SIZE&quot;</span><span class="p">])</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="c1"># Log initialization info</span>
    <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training started with </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2"> processes across </span><span class="si">{</span><span class="n">world_size</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2"> nodes.&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2"> GPUs per node.&quot;</span><span class="p">)</span>
    <span class="c1"># Load the CIFAR-100 dataset with DistributedSampler</span>
    <span class="n">per_gpu_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">//</span> <span class="n">world_size</span>  <span class="c1"># Divide global batch size across GPUs</span>
    <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span>
        <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR100</span><span class="p">(</span>
            <span class="n">root</span><span class="o">=</span><span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/shared/data/&quot;</span><span class="p">,</span>
            <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">download</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">train_loader</span><span class="p">,</span> <span class="n">test_loader</span> <span class="o">=</span> <span class="n">load_cifar100</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">per_gpu_batch_size</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span>
    <span class="p">)</span>
    <span class="c1"># Create the model and wrap it with DDP</span>
    <span class="n">num_classes</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># CIFAR-100 has 100 classes</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">WideResNet</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">local_rank</span><span class="p">])</span>
    <span class="c1"># Define loss function and optimizer</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
    <span class="c1"># Initialize gradient scaler for mixed precision</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">GradScaler</span><span class="p">()</span>
    <span class="n">val_accuracy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">total_time</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_images</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Total images processed globally</span>
    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">train_sampler</span><span class="o">.</span><span class="n">set_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>  <span class="c1"># Set the sampler epoch for shuffling</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="c1"># Train the model for one epoch</span>
        <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Zero the gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># Forward pass with mixed precision</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="c1"># Backward pass and optimization with scaled gradients</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="c1"># Synchronize all processes</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
        <span class="n">epoch_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span>
        <span class="n">total_time</span> <span class="o">+=</span> <span class="n">epoch_time</span>
        <span class="c1"># Compute throughput (images per second for this epoch)</span>
        <span class="n">images_per_sec</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">/</span> <span class="n">epoch_time</span>
        <span class="n">total_images</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">args</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="c1"># Compute validation accuracy and loss</span>
        <span class="n">v_accuracy</span><span class="p">,</span> <span class="n">v_loss</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="c1"># Average validation metrics across all GPUs</span>
        <span class="n">v_accuracy_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">v_accuracy</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">v_loss_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">v_loss</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">v_accuracy_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">AVG</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">v_loss_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">AVG</span><span class="p">)</span>
        <span class="c1"># Print metrics only from the main process</span>
        <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">epochs</span><span class="si">}</span><span class="s2"> completed in </span><span class="si">{</span><span class="n">epoch_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation Loss: </span><span class="si">{</span><span class="n">v_loss_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Validation Accuracy: </span><span class="si">{</span><span class="n">v_accuracy_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch Throughput: </span><span class="si">{</span><span class="n">images_per_sec</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> images/second&quot;</span><span class="p">)</span>
        <span class="c1"># Early stopping</span>
        <span class="n">val_accuracy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v_accuracy_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">patience</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">acc</span> <span class="o">&gt;=</span> <span class="n">args</span><span class="o">.</span><span class="n">target_accuracy</span> <span class="k">for</span> <span class="n">acc</span> <span class="ow">in</span> <span class="n">val_accuracy</span><span class="p">[</span><span class="o">-</span><span class="n">args</span><span class="o">.</span><span class="n">patience</span><span class="p">:]):</span>
            <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target accuracy reached. Early stopping after epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
            <span class="k">break</span>
    <span class="c1"># Log total training time and summary</span>
    <span class="k">if</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">throughput</span> <span class="o">=</span> <span class="n">total_images</span> <span class="o">/</span> <span class="n">total_time</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training Summary:&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total training time: </span><span class="si">{</span><span class="n">total_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> seconds&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Throughput: </span><span class="si">{</span><span class="n">throughput</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2"> images/second&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of nodes: </span><span class="si">{</span><span class="n">world_size</span><span class="w"> </span><span class="o">//</span><span class="w"> </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of GPUs per node: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total GPUs used: </span><span class="si">{</span><span class="n">world_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training completed successfully.&quot;</span><span class="p">)</span>
    <span class="c1"># Clean up the distributed environment</span>
    <span class="n">destroy_process_group</span><span class="p">()</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main_worker</span><span class="p">()</span>
</pre></div>
</div>
<section id="job-script-for-multi-gpu-training">
<h4>Job Script for Multi GPU Training<a class="headerlink" href="#job-script-for-multi-gpu-training" title="Link to this heading"></a></h4>
<p>To run the training on multiple GPUs, we can use the same job script mentioned
earlier, but specify a higher number of GPUs.</p>
<p>When using <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> for a single-node setup, you need to include the
<code class="docutils literal notranslate"><span class="pre">--standalone</span></code> argument. However, this argument is not required for a multi-node
setup. This job script is designed to train the model across multiple GPUs on a
single node. In this script, we explicitly define the path to the torchrun
executable using the following line:</p>
<p><code class="docutils literal notranslate"><span class="pre">TORCHRUN_PATH=&quot;/usr/local/bin/torchrun&quot;</span></code></p>
<p>While experimenting, we encountered cases where the torchrun executable was not
recognized unless its full path was explicitly specified. Defining the
TORCHRUN_PATH in the script resolves this issue. However, this configuration may
vary depending on your working environment:</p>
<p>If the torchrun executable is already in your $PATH, explicitly setting the path
may not be necessary.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=simple_nn_training</span>
<span class="c1">#SBATCH --account=&lt;project_number&gt;</span>
<span class="c1">#SBATCH --output=singlenode_with_container_%j.out</span>
<span class="c1">#SBATCH --error=singlenode_with_container_%j.err</span>
<span class="c1">#SBATCH --time=01:00:00</span>
<span class="c1">#SBATCH --partition=accel</span>
<span class="c1">#SBATCH --nodes=1     # Use one compute node</span>
<span class="c1">##SBATCH --nodelist=x1000c2s4b1n0</span>
<span class="c1">#SBATCH --ntasks-per-node=1 #  Single task per node</span>
<span class="c1">#SBATCH --cpus-per-gpu=72  # Reserve enough CPU cores for full workload with each GPU</span>
<span class="c1">#SBATCH --mem-per-gpu=110G   # Request 110 GB of CPU RAM per GPU</span>
<span class="c1">#SBATCH --gpus-per-node=4   # Reserve 4 GPUs on node</span>

<span class="c1"># Path to the container</span>
<span class="nv">CONTAINER_PATH</span><span class="o">=</span><span class="s2">&quot;/cluster/work/support/container/pytorch_nvidia_25.05_arm64.sif&quot;</span>

<span class="c1"># Path to the training script</span>
<span class="nv">TRAINING_SCRIPT</span><span class="o">=</span><span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/simple_nn_project/singlenode.py --batch-size 1024 --epochs 100 --base-lr 0.04 --target-accuracy 0.95 --patience 2&quot;</span>

<span class="c1"># Bind directories</span>
<span class="nv">BIND_DIR</span><span class="o">=</span><span class="s2">&quot;/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private&quot;</span>

<span class="c1"># Set PYTHONPATH to include the shared directory</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span>/cluster/work/projects/&lt;project_number&gt;/&lt;user_name&gt;/PyTorch/private/shared:<span class="nv">$PYTHONPATH</span>

<span class="c1"># Explicitly specify the full path to torchrun</span>
<span class="nv">TORCHRUN_PATH</span><span class="o">=</span><span class="s2">&quot;/usr/local/bin/torchrun&quot;</span>

apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span><span class="nv">$CONTAINER_PATH</span><span class="w"> </span>which<span class="w"> </span>torchrun
<span class="c1"># Start GPU utilization monitoring in the background</span>
<span class="nv">GPU_LOG_FILE</span><span class="o">=</span><span class="s2">&quot;gpu_utilization_multinode_container.log&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Starting GPU utilization monitoring...&quot;</span>
nvidia-smi<span class="w"> </span>--query-gpu<span class="o">=</span>timestamp,index,name,utilization.gpu,utilization.memory,memory.total,memory.used<span class="w"> </span>--format<span class="o">=</span>csv<span class="w"> </span>-l<span class="w"> </span><span class="m">5</span><span class="w"> </span>&gt;<span class="w"> </span><span class="nv">$GPU_LOG_FILE</span><span class="w"> </span><span class="p">&amp;</span>

<span class="c1"># Run the training script with torchrun inside the container</span>
apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>--bind<span class="w"> </span><span class="nv">$BIND_DIR</span><span class="w"> </span><span class="nv">$CONTAINER_PATH</span><span class="w"> </span><span class="nv">$TORCHRUN_PATH</span><span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="nv">$SLURM_JOB_NUM_NODES</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="nv">$SLURM_GPUS_ON_NODE</span><span class="w"> </span><span class="nv">$TRAINING_SCRIPT</span>

<span class="c1"># Stop GPU utilization monitoring</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Stopping GPU utilization monitoring...&quot;</span>
pkill<span class="w"> </span>-f<span class="w"> </span><span class="s2">&quot;nvidia-smi --query-gpu&quot;</span>
</pre></div>
</div>
<p>Output of the training is shown below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Epoch<span class="w"> </span><span class="m">95</span>/100<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.280<span class="w"> </span>seconds
Validation<span class="w"> </span>Loss:<span class="w"> </span><span class="m">1</span>.0222,<span class="w"> </span>Validation<span class="w"> </span>Accuracy:<span class="w"> </span><span class="m">0</span>.7416
Epoch<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">38409</span>.065<span class="w"> </span>images/second
Epoch<span class="w"> </span><span class="m">96</span>/100<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.271<span class="w"> </span>seconds
Validation<span class="w"> </span>Loss:<span class="w"> </span><span class="m">1</span>.0204,<span class="w"> </span>Validation<span class="w"> </span>Accuracy:<span class="w"> </span><span class="m">0</span>.7439
Epoch<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">38665</span>.960<span class="w"> </span>images/second
Epoch<span class="w"> </span><span class="m">97</span>/100<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.268<span class="w"> </span>seconds
Validation<span class="w"> </span>Loss:<span class="w"> </span><span class="m">1</span>.0401,<span class="w"> </span>Validation<span class="w"> </span>Accuracy:<span class="w"> </span><span class="m">0</span>.7393
Epoch<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">38766</span>.180<span class="w"> </span>images/second
Epoch<span class="w"> </span><span class="m">98</span>/100<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.276<span class="w"> </span>seconds
Validation<span class="w"> </span>Loss:<span class="w"> </span><span class="m">1</span>.0070,<span class="w"> </span>Validation<span class="w"> </span>Accuracy:<span class="w"> </span><span class="m">0</span>.7447
Epoch<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">38512</span>.740<span class="w"> </span>images/second
Epoch<span class="w"> </span><span class="m">99</span>/100<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.273<span class="w"> </span>seconds
Validation<span class="w"> </span>Loss:<span class="w"> </span><span class="m">1</span>.0075,<span class="w"> </span>Validation<span class="w"> </span>Accuracy:<span class="w"> </span><span class="m">0</span>.7435
Epoch<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">38609</span>.904<span class="w"> </span>images/second
Epoch<span class="w"> </span><span class="m">100</span>/100<span class="w"> </span>completed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.281<span class="w"> </span>seconds
Validation<span class="w"> </span>Loss:<span class="w"> </span><span class="m">1</span>.0194,<span class="w"> </span>Validation<span class="w"> </span>Accuracy:<span class="w"> </span><span class="m">0</span>.7429
Epoch<span class="w"> </span>Throughput:<span class="w"> </span><span class="m">38355</span>.399<span class="w"> </span>images/second

Training<span class="w"> </span>Summary:
Total<span class="w"> </span>training<span class="w"> </span>time:<span class="w"> </span><span class="m">131</span>.142<span class="w"> </span>seconds
Throughput:<span class="w"> </span><span class="m">37479</span>.949<span class="w"> </span>images/second
Number<span class="w"> </span>of<span class="w"> </span>nodes:<span class="w"> </span><span class="m">1</span>
Number<span class="w"> </span>of<span class="w"> </span>GPUs<span class="w"> </span>per<span class="w"> </span>node:<span class="w"> </span><span class="m">4</span>
Total<span class="w"> </span>GPUs<span class="w"> </span>used:<span class="w"> </span><span class="m">4</span>
Training<span class="w"> </span>completed<span class="w"> </span>successfully.
</pre></div>
</div>
<p>Note that by using four GPUs, the throughput is <code class="docutils literal notranslate"><span class="pre">37479.949</span> <span class="pre">images/second</span></code> and
the training time is <code class="docutils literal notranslate"><span class="pre">131.142</span> <span class="pre">seconds</span></code>. This indicates that we achieved
superlinear scaling with a 4-GPU setup, which is highly efficient with a speedup
factor of <code class="docutils literal notranslate"><span class="pre">15.21</span></code> and a scaling efficiency of <code class="docutils literal notranslate"><span class="pre">374.1%</span></code>.</p>
</section>
</section>
<section id="multi-node-setup">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Multi-Node Setup</a><a class="headerlink" href="#multi-node-setup" title="Link to this heading"></a></h3>
<p>We have successfully tested the multi-node setup using native installation and
verified that it scales effectively. However, to fully leverage Slingshot within
the containerized environment, we discovered that it is necessary to install a
custom NCCL version integrated with the AWS-OFI plugin. This aspect is still under
development, and we will update this documentation with the necessary details
once the process is finalized. It is likely that the container will soon include
the required components.</p>
<p>In the meantime, if you are interested in learning how we achieved functionality
with the native installation, please feel free to reach out to us. We will be
happy to provide additional details and guidance.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="PyTorchProfilerMultiGpu.html" class="btn btn-neutral float-left" title="Profiling a Resnet-18 model on Multiple GPUs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Sigma2/NRIS. Text shared under CC-BY 4.0 license.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>