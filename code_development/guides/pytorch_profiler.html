

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Profiling GPU-accelerated Deep Learning &mdash; Sigma2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/nris.css?v=69e7a171" />
      <link rel="stylesheet" type="text/css" href="../../_static/universal-navbar.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/statuspal.css" />

  
    <link rel="shortcut icon" href="../../_static/nris.ico"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script async="async" src="https://siteimproveanalytics.com/js/siteanalyze_6036825.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/statuspal_widget.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">
<!-- Send url to parent when displayed as iframe -->
<script>
    const valid_orign_url = "https://www.sigma2.no"
    window.addEventListener('message', function(event) {
        if (event.data === 'getDocumentationIframeUrl' && event.origin.startsWith(valid_orign_url)) {
            // path only (/path/example.html)
            const path = window.location.pathname
            // query string (including the initial ? symbol)
            const search = window.location.search
            // Returns the hash (including the initial # symbol)
            const hash = window.location.hash
            const newUrl = path + search + hash;
            event.source.postMessage(newUrl, event.origin)
        }
    })

</script>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Sigma2/NRIS documentation
              <img src="../../_static/NRIS Logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Policies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../code-of-conduct.html">Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/acceptable-use-policy">User Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/security-policy.html">Security policy for Sigma2 infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/sharing_files.html">Data handling and storage policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/licenses.html">Licence and access policies</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/data-policy">Data Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/data-decommissioning-policies">Data decommissioning policies</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/central-data-library-policy">Central Data Library Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/policies">Overview of Sigma2 Policies</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/support_line.html">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/extended_support.html">Extended support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/how_to_write_good_support_requests.html">Writing good support requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/qa-sessions.html">Open Question &amp; Answer Sessions for All Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/lost_forgotten_password.html">Lost, expiring or changing passwords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/two_factor_authentication.html">One-time-pad (OTP) / Two-factor authentication</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/project-leader-handbook">Project Leader Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/events.html">Training events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/notes_qa.html">Questions, Answers and Feedbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/videos.html">Training Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/short_instructions.html">Short Instructions Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/material.html">Training materials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/opslog.html">Status and maintenance of systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/applying_account.html">How do I get an account?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/applying_resources.html">Applying for computing and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/file_transfer.html">File transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/editing_files.html">Editing files</a></li>
<li class="toctree-l1"><a class="reference internal" href="vs_code/connect_to_server.html">Connecting to a system with Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ssh.html">SSH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ssh.html#common-ssh-errors">Common SSH errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ood.html">Open OnDemand</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/R.html">First R calculation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data and Storage Services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/nird_dp.html">NIRD Data Peak</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/nird_dl.html">NIRD Data Lake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/backup_lmd.html">NIRD Backup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/cdl.html">(NIRD) Central Data Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_archive/user-guide.html">NIRD Research Data Archive (NIRD RDA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_service_platform/overview_nird_service_platform.html">NIRD Service Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Storage Resources and Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird_lmd.html">NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/clusters.html">Storage areas on HPC clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/quota.html">Storage quota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/backup.html">Backup on Betzy, Saga, and NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/performance.html">Optimizing storage performance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HPC usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/migration2metacenter.html">Migration to an NRIS HPC machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing/responsible-use.html">Using shared resources responsibly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/overview.html">Running jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/internet-login-compute-nodes.html">Login nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/internet-login-compute-nodes.html#compute-nodes">Compute nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing/tuning-applications.html">Tuning applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides_llm.html">Running LLM Models in a Cluster Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compute resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/hardware_overview.html">Overview over our machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/betzy.html">Betzy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/olivia.html">Olivia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/saga.html">Saga</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/lumi.html">LUMI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../software/modulescheme.html">Software module scheme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/installed_software.html">Installed software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/userinstallsw.html">Installing software as user</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/appguides.html">Application guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/eessi.html">EESSI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools and Additional services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../nird_toolkit/overview.html">NIRD Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/course_resources.html">CRaaS - Course Resources as a Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code development and tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Code development and tutorials</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Sigma2/NRIS documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Profiling GPU-accelerated Deep Learning</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="profiling-gpu-accelerated-deep-learning">
<span id="pytochprofiler"></span><h1><a class="toc-backref" href="#id13" role="doc-backlink">Profiling GPU-accelerated Deep Learning</a><a class="headerlink" href="#profiling-gpu-accelerated-deep-learning" title="Link to this heading"></a></h1>
<p>We present an introduction to profiling GPU-accelerated Deep Learning (DL) models using <a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a>. Profiling is a necessary step in code development, as it permits identifying bottlenecks in an application. This in turn helps optimize applications, thus improving performance.</p>
<p>This introduction is limited to profiling DL-application that runs on a single GPU. By the end of this guide, readers are expected to learn about:</p>
<ul class="simple">
<li><p>Defining the concept and the architecture of PyTorch Profiler.</p></li>
<li><p>Setting up PyTorch profiler on an HPC system.</p></li>
<li><p>Profiling a PyTorch-based application.</p></li>
<li><p>Visualizing the output data on a web browser with the Tensorboard plugin, in particular, the metrics:</p>
<ul>
<li><p>GPU usage</p></li>
<li><p>GPU Kernel view</p></li>
<li><p>Memory view</p></li>
<li><p>Trace view</p></li>
<li><p>Module view</p></li>
</ul>
</li>
</ul>
<nav class="contents" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#profiling-gpu-accelerated-deep-learning" id="id13">Profiling GPU-accelerated Deep Learning</a></p>
<ul>
<li><p><a class="reference internal" href="#what-is-pytorch-profiler" id="id14">What is PyTorch Profiler</a></p></li>
<li><p><a class="reference internal" href="#setup-pytorch-profiler-in-an-hpc-system" id="id15">Setup Pytorch profiler in an HPC system</a></p></li>
<li><p><a class="reference internal" href="#case-example-profiling-a-resnet-18-model" id="id16">Case example: Profiling a Resnet 18 model</a></p>
<ul>
<li><p><a class="reference internal" href="#visualization-on-a-web-browser" id="id17">Visualization on a web browser</a></p></li>
<li><p><a class="reference internal" href="#on-saga-cluster" id="id18">On Saga cluster</a></p></li>
<li><p><a class="reference internal" href="#performance-metrics" id="id19">Performance metrics</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#launching-a-pytorch-based-application" id="id20">Launching a PyTorch-based application</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id21">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#relevant-links" id="id22">Relevant links</a></p></li>
</ul>
</nav>
<section id="what-is-pytorch-profiler">
<span id="profiler"></span><h2><a class="toc-backref" href="#id14" role="doc-backlink">What is PyTorch Profiler</a><a class="headerlink" href="#what-is-pytorch-profiler" title="Link to this heading"></a></h2>
<p>In general, the concept of profiling is based on statistical sampling, by collecting data at a regular time interval. Here, a profiler tool offers an overview of the execution time attributed to the instructions of a program. In particular, it provides the execution time for each function; in addition to how many times each function has been called. Profiling analysis helps to understand the structure of a code, and more importantly, it helps to identify bottlenecks in an application. Examples of bottlenecks might be related to memory usage and/or identifying functions/libraries that use the majority of the computing time.</p>
<p>PyTorch Profiler is a profiling tool for analyzing Deep Learning models, which is based on collecting performance metrics during training and inference. The profiler is built inside the PyTorch API (cf. <a class="reference internal" href="#fig-arch-profiler"><span class="std std-ref">Fig 1</span></a>), and thus there is no need for installing additional packages. It is a dynamic tool as it is based on gathering statistical data during the running procedure of a training model.</p>
<figure class="align-center" id="id5">
<span id="fig-arch-profiler"></span><a class="reference internal image-reference" href="../../_images/fig00.png"><img alt="../../_images/fig00.png" src="../../_images/fig00.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 1: A simplified version of the architecture of PyTorch Profiler. A complete picture of the architecture can be found [here](#https://www.youtube.com/watch?v=m6ouC0XMYnc&amp;ab_channel=PyTorch) (see the slide at 23:00 min).</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>As shown in the figure, the PyTorch API contains a Python API and a C++ API. For simplicity we highlight only the necessary components for understanding the functionality of PyTorch profiler, which integrates the following: (i) aTen operators, which are libraries of tensor operators for PyTorch and are GPU-accelerated with CUDA; (ii) Kineto library designed specifically for profiling and tracing PyTorch models; and (iii) LibCUPTI (CUDA Profiling Tool Interface), which is a library that provides an interface for profiling and tracing CUDA-based application (low-level profiling). The last two libraries provide an interface for collecting and analyzing the performance data at the level of GPU.</p>
<p>Here we list the performance metrics provided by the profiler, which we shall describe in <a class="reference internal" href="#performance-metrics"><span class="std std-ref">Section</span></a>:</p>
<ul class="simple">
<li><p>GPU usage</p></li>
<li><p>Tensor cores usage (if it is enabled)</p></li>
<li><p>GPU Kernel view</p></li>
<li><p>Memory view</p></li>
<li><p>Trace view</p></li>
<li><p>Module view</p></li>
</ul>
<p>Further details are provided in these <a class="reference external" href="https://github.com/HichamAgueny/Profiling-GPU-accelerated-DL">slides</a>.</p>
</section>
<section id="setup-pytorch-profiler-in-an-hpc-system">
<span id="id1"></span><h2><a class="toc-backref" href="#id15" role="doc-backlink">Setup Pytorch profiler in an HPC system</a><a class="headerlink" href="#setup-pytorch-profiler-in-an-hpc-system" title="Link to this heading"></a></h2>
<p>In this section, we describe how to set up PyTorch using a singularity container.</p>
<ul class="simple">
<li><p><strong>Step 1</strong>: Pull and convert a docker image to a singularity image format:
e.g. from the <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">NVIDIA NGC container</a></p></li>
</ul>
<p>Note that when pulling docker containers using singularity, the conversion can be quite heavy and the singularity cache directory in <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> space becomes full of temporary files. To speed up the conversion and avoid storing temporary files, one can first run these lines:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>mkdir<span class="w"> </span>-p<span class="w"> </span>/tmp/<span class="nv">$USER</span><span class="w"> </span>
<span class="gp">$ </span><span class="nb">export</span><span class="w"> </span><span class="nv">SINGULARITY_TMPDIR</span><span class="o">=</span>/tmp/<span class="nv">$USER</span><span class="w"> </span>
<span class="gp">$ </span><span class="nb">export</span><span class="w"> </span><span class="nv">SINGULARITY_CACHEDIR</span><span class="o">=</span>/tmp/<span class="nv">$USER</span>
</pre></div>
</div>
<p>and then pull the container</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span>singularity<span class="w"> </span>pull<span class="w"> </span>docker://nvcr.io/nvidia/pytorch:22.12-py3
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Step 2</strong>: Launch the singularity container</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>-B<span class="w"> </span><span class="si">${</span><span class="nv">MyEx</span><span class="si">}</span><span class="w"> </span>pytorch_22.12-py3.sif<span class="w"> </span>python<span class="w"> </span><span class="si">${</span><span class="nv">MyEx</span><span class="si">}</span>/resnet18_api.py
</pre></div>
</div>
<p>Here the container is mounted to the path <code class="docutils literal notranslate"><span class="pre">${MyEx}</span></code>, where the Python application is located. An example of a Slurm script that launches a singularity container is provided in the <a class="reference internal" href="#launching-a-pytorch-based-application"><span class="std std-ref">Section</span></a>.</p>
</section>
<section id="case-example-profiling-a-resnet-18-model">
<span id="id2"></span><h2><a class="toc-backref" href="#id16" role="doc-backlink">Case example: Profiling a Resnet 18 model</a><a class="headerlink" href="#case-example-profiling-a-resnet-18-model" title="Link to this heading"></a></h2>
<p>We consider the Resnet 18 model as an example to illustarte profiling with <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorch profiler</a>. Here we list the lines of code required to enable profiling with <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorch Profiler</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA],
    schedule=torch.profiler.schedule(
        wait=1,
        warmup=1,
        active=2),
    on_trace_ready=torch.profiler.tensorboard_trace_handler(‘./out&#39;, worker_name=‘profiler&#39;),
    record_shapes=True,
    profile_memory=True, 
    with_stack=True
) as prof:
</pre></div>
</div>
<p>To be incorporated just above the training loop</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#training step for each batch of input data</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
          <span class="o">.</span>
          <span class="o">.</span>
          <span class="o">.</span>
          <span class="o">.</span>
       <span class="k">if</span> <span class="n">step</span> <span class="o">+</span><span class="mi">1</span><span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is a code example of the Resnet18 model, in which profiling is enabled. The code is adapted from the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorch tutorial</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">#import all the necessary libraries </span>
<span class="linenos"> 2</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="linenos"> 3</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span>
<span class="linenos"> 4</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span>
<span class="linenos"> 5</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.profiler</span>
<span class="linenos"> 6</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.data</span>
<span class="linenos"> 7</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.datasets</span>
<span class="linenos"> 8</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span>
<span class="linenos"> 9</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">T</span>
<span class="linenos">10</span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ResNet18_Weights</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1">#prepare input data and transform it</span>
<span class="linenos">13</span><span class="n">transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">T</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> 
<span class="linenos">14</span>    <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="c1">#trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True,transform=transform)</span>
<span class="linenos">17</span><span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
<span class="linenos">18</span>    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="c1"># use dataloader to launch each batch</span>
<span class="linenos">21</span><span class="c1">#trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,shuffle=True, num_workers=4)</span>
<span class="linenos">22</span><span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">23</span>
<span class="linenos">24</span><span class="c1"># Create a Resnet model, loss function, and optimizer objects. To run on GPU, move model and loss to a GPU device</span>
<span class="linenos">25</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="c1">#model = torchvision.models.resnet18(pretrained=True).cuda(device)</span>
<span class="linenos">28</span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">29</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">30</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="linenos">31</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="linenos">32</span>
<span class="linenos">33</span><span class="c1">#Use profiler</span>
<span class="hll"><span class="linenos">34</span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
</span><span class="hll"><span class="linenos">35</span>    <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
</span><span class="hll"><span class="linenos">36</span>        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">37</span>        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
</span><span class="hll"><span class="linenos">38</span>    <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span>
</span><span class="hll"><span class="linenos">39</span>        <span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">40</span>        <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">41</span>        <span class="n">active</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
</span><span class="hll"><span class="linenos">42</span>    <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">tensorboard_trace_handler</span><span class="p">(</span><span class="s1">&#39;./out&#39;</span><span class="p">,</span> <span class="n">worker_name</span><span class="o">=</span><span class="s1">&#39;worker4&#39;</span><span class="p">),</span>
</span><span class="hll"><span class="linenos">43</span>    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">44</span>    <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># This will take 1 to 2 minutes. Setting it to False could greatly speedup.</span>
</span><span class="hll"><span class="linenos">45</span>    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
</span><span class="hll"><span class="linenos">46</span><span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
</span><span class="hll"><span class="linenos">47</span><span class="c1">#include</span>
</span><span class="hll"><span class="linenos">48</span>    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
</span><span class="hll"><span class="linenos">49</span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;step:</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">))</span>
</span><span class="hll"><span class="linenos">50</span>        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">51</span>
</span><span class="hll"><span class="linenos">52</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">53</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">54</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">55</span>        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">56</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">57</span>        <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
</span><span class="hll"><span class="linenos">58</span>            <span class="k">break</span>
</span><span class="hll"><span class="linenos">59</span>        <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span class="linenos">60</span>
<span class="linenos">61</span><span class="nb">print</span><span class="p">()</span>
<span class="linenos">62</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;--Print GPU: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos">63</span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
</pre></div>
</div>
<p>For reference, we provide here the same application but without enabling profiling. The code is adapted from the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">PyTorch tutorial</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">#import all the necessary libraries </span>
<span class="linenos"> 2</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="linenos"> 3</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span>
<span class="linenos"> 4</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span>
<span class="linenos"> 5</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.profiler</span>
<span class="linenos"> 6</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.data</span>
<span class="linenos"> 7</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.datasets</span>
<span class="linenos"> 8</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span>
<span class="linenos"> 9</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">T</span>
<span class="linenos">10</span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ResNet18_Weights</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1">#prepare input data and transform it</span>
<span class="linenos">13</span><span class="n">transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">T</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> 
<span class="linenos">14</span>    <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))])</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="c1">#trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True,transform=transform)</span>
<span class="linenos">17</span><span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
<span class="linenos">18</span>    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="c1"># use dataloader to launch each batch</span>
<span class="linenos">21</span><span class="c1">#trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,shuffle=True, num_workers=4)</span>
<span class="linenos">22</span><span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">23</span>
<span class="linenos">24</span><span class="c1"># Create a Resnet model, loss function, and optimizer objects. To run on GPU, move model and loss to a GPU device</span>
<span class="linenos">25</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="c1">#model = torchvision.models.resnet18(pretrained=True).cuda(device)</span>
<span class="linenos">28</span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">29</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">30</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="linenos">31</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="linenos">32</span>
<span class="linenos">33</span><span class="c1">#Define the training step for each batch of input data.</span>
<span class="hll"><span class="linenos">34</span><span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
</span><span class="hll"><span class="linenos">35</span>    <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">36</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">37</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">38</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">39</span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">40</span>    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span class="linenos">41</span>
<span class="linenos">42</span><span class="nb">print</span><span class="p">()</span>
<span class="linenos">43</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;--Print GPU: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos">44</span><span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">())</span>
</pre></div>
</div>
<p>In the lines of code defined above, one needs to specify the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">setting for profiling</a>. The latter can be split into three main parts:</p>
<ul>
<li><p>Import <code class="docutils literal notranslate"><span class="pre">torch.profiler</span></code></p></li>
<li><p>Specify the profiler context: i.e. which kind of <strong>activities</strong> one can profile. e.g. CPU activities (i.e. <code class="docutils literal notranslate"><span class="pre">torch.profiler.ProfilerActivity.CPU</span></code>), GPU activities (i.e. <code class="docutils literal notranslate"><span class="pre">torch.profiler.ProfilerActivity.CUDA</span></code>) or both activities.</p></li>
<li><p>Define the <strong>schedule</strong>; in particular, the following options can be specified:
—<em>wait=l</em>: Profiling is disabled for the first <code class="docutils literal notranslate"><span class="pre">l</span></code> steps. This is relevant if the training takes a longer time, and that profiling the entire training loop is not desired. Here, one can wait for <code class="docutils literal notranslate"><span class="pre">l</span></code> steps before the profiling gets started.</p>
<p>—<em>warmup=N</em>: The profiler collects data after N steps for tracing.</p>
<p>—<em>active=M</em>: Events will be recorded for tracing during the active steps. This is useful to avoid tracing a lot of events, which might cause issues with loading the data.</p>
</li>
<li><p>Additional options: Trace, record shape, profile memory, with stack, could be enabled.</p></li>
</ul>
<p>Note that, in the <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">loop</span></code> (i.e. <em>the training loop</em>), one needs to call the profile step (<code class="docutils literal notranslate"><span class="pre">prof.step()</span></code>), in order to collect all the necessary inputs, which in turn will generate data that can be viewed with the Tensorboard plugin. In the end, the output of profiling will be saved in the <code class="docutils literal notranslate"><span class="pre">/out</span></code> directory.</p>
<p>Note that a good practice of profiling should be based on the following: first one can start profiling for a large training loop, and once we identify the bottleneck, then we can select a few iterations for re-profiling and tuning the application. This should be followed by optimising the application and eventually re-profiling to check the impact of the optimisation.</p>
<section id="visualization-on-a-web-browser">
<span id="visualisation-on-a-web-browser"></span><h3><a class="toc-backref" href="#id17" role="doc-backlink">Visualization on a web browser</a><a class="headerlink" href="#visualization-on-a-web-browser" title="Link to this heading"></a></h3>
<p>To view the output data generated from the profiling process, one needs to install TensorBord. This can be done for instance in a virtual environment. Here we desccribe a step-by-step guide of the installation:</p>
<ul class="simple">
<li><p><strong>Step 1</strong>: Load a Python model, create and activate a virtual environment.
Load a Python module. e.g.: <code class="docutils literal notranslate"><span class="pre">module</span></code> load python/3.9.6-GCCcore-11.2.0`</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mkdir</span> <span class="pre">Myenv</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">–m</span> <span class="pre">venv</span> <span class="pre">Myenv</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">Myenv/bin/activate</span></code></p></li>
<li><p><strong>Step 2</strong>: Install TensorBoard Plugin via pip wheel packages using the following command (see also <a class="reference external" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html">here</a>):</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">–m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch_tb_profiler</span></code></p></li>
<li><p><strong>Step 3</strong>: Run Tensorboard using the command:</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">tensorboard --logdir=./out --bind_all</span>
</pre></div>
</div>
<p>This will generate a local address having a specific registered or private port, as shown in <a class="reference internal" href="#fig-tensorboard"><span class="std std-ref">Figure</span></a>. Note that in HPC systems, direct navigation to the generated address is blocked by firewalls. Therefore, connecting to an internal network from outside can be done via a mechanism called <a class="reference external" href="https://www.ssh.com/academy/ssh/tunneling-example#local-forwarding">local port forwarding</a>. As stated in the <a class="reference external" href="https://www.ssh.com/academy/ssh/tunneling-example#local-forwarding">SSH documentation</a> “Local forwarding is used to forward a port from the client machine to the server machine”.</p>
<p>The syntax for local forwarding, which is configured using the option <code class="docutils literal notranslate"><span class="pre">–L</span></code>, can be written as, e.g.:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ssh -L 6009:localhost:6006 username@server.address.com</span>
</pre></div>
</div>
<p>This syntax enables opening a connection to the jump server <code class="docutils literal notranslate"><span class="pre">username&#64;server.address.com</span></code>, and forwarding any connection from port 6009 on the local machine to port 6006 on the server <code class="docutils literal notranslate"><span class="pre">username&#64;server.address.com</span></code>.</p>
<p>Lastly, the local address <code class="docutils literal notranslate"><span class="pre">http://localhost:6009/</span></code> can be viewed in a Chrome or Firefox browser.</p>
</section>
<section id="on-saga-cluster">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">On Saga cluster</a><a class="headerlink" href="#on-saga-cluster" title="Link to this heading"></a></h3>
<p>Here is an example about viewing data using <code class="docutils literal notranslate"><span class="pre">TensorBoard</span></code> on Saga. We assume that TensorBoard plugin is installed in a virtual environment, which we name <code class="docutils literal notranslate"><span class="pre">Myenv</span></code> as described above. Here are main steps:</p>
<p><strong>Step 1</strong>: Source the virtual environment</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span><span class="nb">source</span><span class="w"> </span>Myenv/bin/activate
</pre></div>
</div>
<p><strong>Step 2</strong>: Run the tensorboard command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span>tensorboard<span class="w"> </span>--logdir<span class="o">=</span>./out<span class="w"> </span>--bind_all<span class="sb">`</span>
</pre></div>
</div>
<p>Note that the profiled data are stored in the <code class="docutils literal notranslate"><span class="pre">out</span></code> folder. Running the command prints out a message that includes</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">...</span>
<span class="go">...</span>
<span class="gp">$</span>TensorBoard<span class="w"> </span><span class="m">2</span>.13.0<span class="w"> </span>at<span class="w"> </span>http://login-3.saga.sigma2.no:6006/
<span class="go">...</span>
</pre></div>
</div>
<p>The output message contains the address of the current login node, which is in <em>our case</em> <code class="docutils literal notranslate"><span class="pre">login-3.saga.sigma2.no</span></code>. This address will be used as a jump server as expressed in the next step.</p>
<p><strong>Step 3</strong>: In a new terminal, run this command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ssh -L 6009:localhost:6006 username@login-3.saga.sigma2.no</span>
</pre></div>
</div>
<p>Note that the port number <code class="docutils literal notranslate"><span class="pre">6006</span></code> is taken form the address <code class="docutils literal notranslate"><span class="pre">login-3.saga.sigma2.no:6006</span></code>.</p>
<p><strong>Step 4</strong>: View the profiled data in a Chrome or Firefox browser</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">http://localhost:6009/</span>
</pre></div>
</div>
<figure class="align-center" id="id6">
<span id="fig-tensorboard"></span><a class="reference internal image-reference" href="../../_images/fig0.png"><img alt="../../_images/fig0.png" src="../../_images/fig0.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 2: Output of running the tensorboar command <cite>tensorboard –logdir=./out –bind_all</cite>.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="performance-metrics">
<span id="id3"></span><h3><a class="toc-backref" href="#id19" role="doc-backlink">Performance metrics</a><a class="headerlink" href="#performance-metrics" title="Link to this heading"></a></h3>
<p>In this section, we provide screenshots of different views of performance metrics stemming from PyTorch Profiler. The metrics include:</p>
<ul class="simple">
<li><p>GPU usage (cf. <a class="reference internal" href="#fig-overview"><span class="std std-ref">Figure 3</span></a>)</p></li>
<li><p>GPU Kernel view (cf. <a class="reference internal" href="#fig-kernel"><span class="std std-ref">Figure 4</span></a>)</p></li>
<li><p>Trace view (cf. <a class="reference internal" href="#fig-trace1"><span class="std std-ref">Figure 5</span></a> and <a class="reference internal" href="#fig-trace2"><span class="std std-ref">Figure 6</span></a>)</p></li>
<li><p>Memory view (cf. <a class="reference internal" href="#fig-memory"><span class="std std-ref">Figure 7</span></a>)</p></li>
<li><p>Module view (cf. <a class="reference internal" href="#fig-module"><span class="std std-ref">Figure 8</span></a>)</p></li>
</ul>
<figure class="align-center" id="id7">
<span id="fig-overview"></span><a class="reference internal image-reference" href="../../_images/fig11.png"><img alt="../../_images/fig11.png" src="../../_images/fig11.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 3: Overview of GPU activities.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id8">
<span id="fig-kernel"></span><a class="reference internal image-reference" href="../../_images/fig21.png"><img alt="../../_images/fig21.png" src="../../_images/fig21.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 4: View of GPU Kernels.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id9">
<span id="fig-trace1"></span><a class="reference internal image-reference" href="../../_images/fig31.png"><img alt="../../_images/fig31.png" src="../../_images/fig31.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 5: View of Trace.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id10">
<span id="fig-trace2"></span><a class="reference internal image-reference" href="../../_images/fig41.png"><img alt="../../_images/fig41.png" src="../../_images/fig41.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 6: View of Trace.</span><a class="headerlink" href="#id10" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id11">
<span id="fig-memory"></span><a class="reference internal image-reference" href="../../_images/fig51.png"><img alt="../../_images/fig51.png" src="../../_images/fig51.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 7: View of Memory usage.</span><a class="headerlink" href="#id11" title="Link to this image"></a></p>
</figcaption>
</figure>
<figure class="align-center" id="id12">
<span id="fig-module"></span><a class="reference internal image-reference" href="../../_images/fig6.png"><img alt="../../_images/fig6.png" src="../../_images/fig6.png" style="width: 600px;" />
</a>
<figcaption>
<p><span class="caption-text">Fig 8: View of Modules.</span><a class="headerlink" href="#id12" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
</section>
<section id="launching-a-pytorch-based-application">
<span id="id4"></span><h2><a class="toc-backref" href="#id20" role="doc-backlink">Launching a PyTorch-based application</a><a class="headerlink" href="#launching-a-pytorch-based-application" title="Link to this heading"></a></h2>
<p>For completeness, we provide an example of a job script that incorporates a PyTorch singularity container. The script can be adapted according to requested computing resources.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>
<span class="c1">#SBATCH --job-name=PyTprofiler</span>
<span class="c1">#SBATCH --account=&lt;project_account&gt;</span>
<span class="c1">#SBATCH --time=00:10:00     #wall-time </span>
<span class="c1">#SBATCH --partition=accel   #partition </span>
<span class="c1">#SBATCH --nodes=1           #nbr of nodes</span>
<span class="c1">#SBATCH --ntasks=1          #nbr of tasks</span>
<span class="c1">#SBATCH --ntasks-per-node=1 #nbr of tasks per nodes (nbr of cpu-cores, MPI-processes)</span>
<span class="c1">#SBATCH --cpus-per-task=1   #nbr of threads</span>
<span class="c1">#SBATCH --gpus=1            #total nbr of gpus</span>
<span class="c1">#SBATCH --gpus-per-node=1   #nbr of gpus per node</span>
<span class="c1">#SBATCH --mem=4G            #main memory</span>
<span class="c1">#SBATCH -o PyTprofiler.out  #slurm output </span>

<span class="c1"># Set up job environment</span>
<span class="nb">set</span><span class="w"> </span>-o<span class="w"> </span>errexit<span class="w"> </span><span class="c1"># exit on any error</span>
<span class="nb">set</span><span class="w"> </span>-o<span class="w"> </span>nounset<span class="w"> </span><span class="c1"># treat unset variables as error</span>

<span class="c1">#define paths</span>
<span class="nv">Mydir</span><span class="o">=</span>&lt;Path-to-Workspace&gt;
<span class="nv">MyContainer</span><span class="o">=</span><span class="si">${</span><span class="nv">Mydir</span><span class="si">}</span>/Container/pytorch_22.12-py3.sif
<span class="nv">MyExp</span><span class="o">=</span><span class="si">${</span><span class="nv">Mydir</span><span class="si">}</span>/examples

<span class="c1">#specify bind paths by setting the environment variable</span>
<span class="c1">#export SINGULARITY_BIND=&quot;${MyExp},$PWD&quot;</span>

<span class="c1">#TF32 is enabled by default in the NVIDIA NGC TensorFlow and PyTorch containers </span>
<span class="c1">#To disable TF32 set the environment variable to 0</span>
<span class="c1">#export NVIDIA_TF32_OVERRIDE=0</span>

<span class="c1">#to run singularity container </span>
singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>-B<span class="w"> </span><span class="si">${</span><span class="nv">MyExp</span><span class="si">}</span>,<span class="nv">$PWD</span><span class="w"> </span><span class="si">${</span><span class="nv">MyContainer</span><span class="si">}</span><span class="w"> </span>python<span class="w"> </span><span class="si">${</span><span class="nv">MyExp</span><span class="si">}</span>/resnet18_with_profiler_api.py

<span class="nb">echo</span><span class="w"> </span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;--Job ID:&quot;</span><span class="w"> </span><span class="nv">$SLURM_JOB_ID</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;--total nbr of gpus&quot;</span><span class="w"> </span><span class="nv">$SLURM_GPUS</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;--nbr of gpus_per_node&quot;</span><span class="w"> </span><span class="nv">$SLURM_GPUS_PER_NODE</span>
</pre></div>
</div>
<p>More details about how to write a job script can be found <a class="reference external" href="https://documentation.sigma2.no/jobs/job_scripts.html">here</a>.</p>
</section>
</section>
<section id="conclusion">
<span id="pytorch-conclusion"></span><h1><a class="toc-backref" href="#id21" role="doc-backlink">Conclusion</a><a class="headerlink" href="#conclusion" title="Link to this heading"></a></h1>
<p>In conclusion, we have provided a guide on how to perform code profiling of GPU-accelerated Deep Learning models using the PyTorch Profiler. The particularity of the profiler relies on its simplicity and ease of use without installing additional packages and with a few lines of code to be added. These lines of code constitute the setting of the profiler, which can be customized according to the desired performance metrics. The profiler provides an overview of metrics; this includes a summary of GPU usage and Tensor cores usage (if it is enabled), this is in addition to an advanced analysis based on the view of GPU kernel, memory usage in time, trace and modules. These features are key elements for identifying bottlenecks in an application. Identifying these bottlenecks has the benefit of optimizing the application to run efficiently and reliably on HPC systems.</p>
</section>
<section id="relevant-links">
<h1><a class="toc-backref" href="#id22" role="doc-backlink">Relevant links</a><a class="headerlink" href="#relevant-links" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a></p>
<p><a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">NVIDIA NGC container</a></p>
<p><a class="reference external" href="https://www.ssh.com/academy/ssh/tunneling-example#local-forwarding">Local port forwarding</a></p>
<p><a class="reference external" href="https://github.com/HichamAgueny/Profiling-GPU-accelerated-DL">Slides</a></p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=m6ouC0XMYnc&amp;amp;ab_channel=PyTorch">PyTorch Profiler video</a></p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, Sigma2/NRIS. Text shared under CC-BY 4.0 license.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>