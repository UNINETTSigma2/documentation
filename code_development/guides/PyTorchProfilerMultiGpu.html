

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Profiling a Resnet-18 model on Multiple GPUs &mdash; Sigma2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/nris.css?v=69e7a171" />
      <link rel="stylesheet" type="text/css" href="../../_static/universal-navbar.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/statuspal.css" />

  
    <link rel="shortcut icon" href="../../_static/nris.ico"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=f281be69"></script>
      <script async="async" src="https://siteimproveanalytics.com/js/siteanalyze_6036825.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/statuspal_widget.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="PyTorch on Olivia" href="pytorch_olivia/PyTorchSingleGpu.html" />
    <link rel="prev" title="Python libraries" href="../guides_python.html" /> 
</head>

<body class="wy-body-for-nav">
<!-- Send url to parent when displayed as iframe -->
<script>
    const valid_orign_url = "https://www.sigma2.no"
    window.addEventListener('message', function(event) {
        if (event.data === 'getDocumentationIframeUrl' && event.origin.startsWith(valid_orign_url)) {
            // path only (/path/example.html)
            const path = window.location.pathname
            // query string (including the initial ? symbol)
            const search = window.location.search
            // Returns the hash (including the initial # symbol)
            const hash = window.location.hash
            const newUrl = path + search + hash;
            event.source.postMessage(newUrl, event.origin)
        }
    })

</script>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Sigma2/NRIS documentation
              <img src="../../_static/NRIS Logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Policies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../code-of-conduct.html">Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/acceptable-use-policy">User Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/security-policy.html">Security policy for Sigma2 infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/sharing_files.html">Data handling and storage policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/licenses.html">Licence and access policies</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/data-policy">Data Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/data-decommissioning-policies">Data decommissioning policies</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/central-data-library-policy">Central Data Library Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/policies">Overview of Sigma2 Policies</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/support_line.html">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/extended_support.html">Extended support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/how_to_write_good_support_requests.html">Writing good support requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/qa-sessions.html">Open Question &amp; Answer Sessions for All Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/lost_forgotten_password.html">Lost, expiring or changing passwords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/two_factor_authentication.html">One-time-pad (OTP) / Two-factor authentication</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/project-leader-handbook">Project Leader Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../training/events.html">Training events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/notes_qa.html">Questions, Answers and Feedbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/videos.html">Training Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/short_instructions.html">Short Instructions Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../training/material.html">Training materials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/opslog.html">Status and maintenance of systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/applying_account.html">How do I get an account?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/applying_resources.html">Applying for computing and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/file_transfer.html">File transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/editing_files.html">Editing files</a></li>
<li class="toctree-l1"><a class="reference internal" href="vs_code/connect_to_server.html">Connecting to a system with Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ssh.html">SSH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ssh.html#common-ssh-errors">Common SSH errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/ood.html">Open OnDemand</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/R.html">First R calculation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data and Storage Services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/nird_dp.html">NIRD Data Peak</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/nird_dl.html">NIRD Data Lake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/backup_lmd.html">NIRD Backup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird/cdl.html">(NIRD) Central Data Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_archive/user-guide.html">NIRD Research Data Archive (NIRD RDA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nird_service_platform/overview_nird_service_platform.html">NIRD Service Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Storage Resources and Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/nird_lmd.html">NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/clusters.html">Storage areas on HPC clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/quota.html">Storage quota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/backup.html">Backup on Betzy, Saga, and NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../files_storage/performance.html">Optimizing storage performance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HPC usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/migration2metacenter.html">Migration to an NRIS HPC machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing/responsible-use.html">Using shared resources responsibly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/overview.html">Running jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/internet-login-compute-nodes.html">Login nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jobs/internet-login-compute-nodes.html#compute-nodes">Compute nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../computing/tuning-applications.html">Tuning applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guides_llm.html">Running LLM Models in a Cluster Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compute resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/hardware_overview.html">Overview over our machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/betzy.html">Betzy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/olivia.html">Olivia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/saga.html">Saga</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hpc_machines/lumi.html">LUMI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../software/modulescheme.html">Software module scheme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/installed_software.html">Installed software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/userinstallsw.html">Installing software as user</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/appguides.html">Application guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../software/eessi.html">EESSI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools and Additional services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../nird_toolkit/overview.html">NIRD Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_help/course_resources.html">CRaaS - Course Resources as a Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code development and tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../overview.html">Code development and tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../overview.html#id1">Code development</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../overview.html#tutorials">Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../guides_gpu.html">GPU programming models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_ml.html">Machine Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_containers_gpu.html">Containers with GPU support</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_monitor_gpu.html">Monitoring GPU accelerated applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../guides_python.html">Python libraries</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Profiling a Resnet-18 model on Multiple GPUs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#code-example-setting-up-ddp-for-the-resnet-18-model">Code Example: Setting Up DDP for the ResNet-18 Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#understanding-the-ddp-setup">Understanding the DDP setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-script-for-utilizing-multiple-gpus">Job Script for Utilizing Multiple GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#performance-metrics">Performance Metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#gpu-usage">1. GPU Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#trace-view">2. Trace View</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-view">3. Memory View</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-view">4. Distributed View</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#conclusion">Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#relevant-links">Relevant links</a></li>
<li class="toctree-l3"><a class="reference internal" href="pytorch_olivia/PyTorchSingleGpu.html">PyTorch on Olivia</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Sigma2/NRIS documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../overview.html">Code development and tutorials</a></li>
      <li class="breadcrumb-item active">Profiling a Resnet-18 model on Multiple GPUs</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="profiling-a-resnet-18-model-on-multiple-gpus">
<span id="pytorch-profiler-multi-gpu"></span><h1>Profiling a Resnet-18 model on Multiple GPUs<a class="headerlink" href="#profiling-a-resnet-18-model-on-multiple-gpus" title="Link to this heading"></a></h1>
<p>In some cases, your model may be too large, or the dataset you are working with might be so extensive that leveraging multiple GPUs becomes necessary to handle the workload efficiently. Running computations in parallel across multiple GPUs can significantly speed up training and improve performance.
This section explains how to extend the previous example of profiling a ResNet-18 model on a single GPU to a multi-GPU setup. We will use PyTorch’s <strong>Distributed Data Parallel (DDP)</strong> module to distribute the workload across multiple GPUs. Additionally, we will utilize the PyTorch Profiler to collect and analyze performance metrics, just as we did for the single GPU case, but now in a multi-GPU environment.</p>
<section id="code-example-setting-up-ddp-for-the-resnet-18-model">
<h2>Code Example: Setting Up DDP for the ResNet-18 Model<a class="headerlink" href="#code-example-setting-up-ddp-for-the-resnet-18-model" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">#import all the necessary libraries </span>
<span class="linenos"> 2</span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="linenos"> 3</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="linenos"> 4</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="linenos"> 5</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span>
<span class="linenos"> 6</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.profiler</span>
<span class="linenos"> 7</span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.utils.data</span>
<span class="linenos"> 8</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.datasets</span>
<span class="linenos"> 9</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span>
<span class="linenos">10</span><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">T</span>
<span class="linenos">11</span><span class="kn">from</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">ResNet18_Weights</span>
<span class="linenos">12</span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">init_process_group</span><span class="p">,</span> <span class="n">destroy_process_group</span>
<span class="linenos">13</span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="linenos">14</span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.distributed</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedSampler</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="k">def</span><span class="w"> </span><span class="nf">ddp_setup</span><span class="p">():</span>
<span class="linenos">17</span>    <span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;nccl&quot;</span><span class="p">)</span>
<span class="linenos">18</span>    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">]))</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="k">def</span><span class="w"> </span><span class="nf">main</span><span class="p">():</span>
<span class="linenos">21</span>    <span class="n">ddp_setup</span><span class="p">()</span>
<span class="linenos">22</span>    <span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">])</span>
<span class="linenos">23</span>    
<span class="linenos">24</span>    <span class="c1"># Prepare data</span>
<span class="linenos">25</span>    <span class="n">transform</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
<span class="linenos">26</span>        <span class="n">T</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="mi">256</span><span class="p">),</span> 
<span class="linenos">27</span>        <span class="n">T</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">),</span> 
<span class="linenos">28</span>        <span class="n">T</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span> 
<span class="linenos">29</span>        <span class="n">T</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="linenos">30</span>    <span class="p">])</span>
<span class="linenos">31</span>    
<span class="linenos">32</span>    <span class="n">trainset</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">CIFAR10</span><span class="p">(</span>
<span class="linenos">33</span>        <span class="n">root</span><span class="o">=</span><span class="s1">&#39;./data&#39;</span><span class="p">,</span> 
<span class="linenos">34</span>        <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
<span class="linenos">35</span>        <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
<span class="linenos">36</span>        <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="linenos">37</span>    <span class="p">)</span>
<span class="linenos">38</span>    
<span class="linenos">39</span>    <span class="n">sampler</span> <span class="o">=</span> <span class="n">DistributedSampler</span><span class="p">(</span><span class="n">trainset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">40</span>    <span class="n">trainloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
<span class="linenos">41</span>        <span class="n">trainset</span><span class="p">,</span> 
<span class="linenos">42</span>        <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="linenos">43</span>        <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
<span class="linenos">44</span>        <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="linenos">45</span>        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="linenos">46</span>        <span class="n">persistent_workers</span><span class="o">=</span><span class="kc">True</span>
<span class="linenos">47</span>    <span class="p">)</span>
<span class="linenos">48</span>    
<span class="linenos">49</span>    <span class="c1"># Model setup</span>
<span class="linenos">50</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">51</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="n">ResNet18_Weights</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">52</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>
<span class="linenos">53</span>    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">54</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="linenos">55</span>    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="linenos">56</span>    
<span class="hll"><span class="linenos">57</span>    <span class="n">prof</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
</span><span class="hll"><span class="linenos">58</span>      <span class="n">activities</span><span class="o">=</span><span class="p">[</span>
</span><span class="hll"><span class="linenos">59</span>        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">60</span>        <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span>
</span><span class="hll"><span class="linenos">61</span>      <span class="p">],</span>
</span><span class="hll"><span class="linenos">62</span>      <span class="n">schedule</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">schedule</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">active</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
</span><span class="hll"><span class="linenos">63</span>      <span class="n">on_trace_ready</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">tensorboard_trace_handler</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;./outgpus&#39;</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">64</span>        <span class="n">worker_name</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;worker</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s1">&#39;</span>
</span><span class="hll"><span class="linenos">65</span>      <span class="p">),</span>
</span><span class="hll"><span class="linenos">66</span>      <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">67</span>      <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span class="hll"><span class="linenos">68</span>      <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
</span><span class="hll"><span class="linenos">69</span>    <span class="p">)</span>
</span><span class="hll"><span class="linenos">70</span>    <span class="n">prof</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">71</span>
</span><span class="hll"><span class="linenos">72</span>    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trainloader</span><span class="p">):</span>
</span><span class="hll"><span class="linenos">73</span>      <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">74</span>    
</span><span class="hll"><span class="linenos">75</span>      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">76</span>      <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">77</span>    
</span><span class="hll"><span class="linenos">78</span>      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">79</span>      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">80</span>      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">81</span>    
</span><span class="hll"><span class="linenos">82</span>      <span class="n">prof</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span><span class="hll"><span class="linenos">83</span>      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> - step: </span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="s2">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</span><span class="hll"><span class="linenos">84</span>
</span><span class="hll"><span class="linenos">85</span>    
</span><span class="hll"><span class="linenos">86</span>      <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;=</span> <span class="mi">10</span><span class="p">:</span>
</span><span class="hll"><span class="linenos">87</span>        <span class="k">break</span>
</span><span class="hll"><span class="linenos">88</span>
</span><span class="hll"><span class="linenos">89</span>    <span class="n">prof</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</span><span class="linenos">90</span>    
<span class="linenos">91</span>    <span class="n">destroy_process_group</span><span class="p">()</span>
<span class="linenos">92</span>
<span class="linenos">93</span><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="linenos">94</span>    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="understanding-the-ddp-setup">
<h2>Understanding the DDP setup<a class="headerlink" href="#understanding-the-ddp-setup" title="Link to this heading"></a></h2>
<p>To enable DDP in our code, we made several modifications to the previous implementation. DDP is a PyTorch module that allows us to parallelize our model across multiple GPUs or even multiple machines. You can learn more about DDP in the <a class="reference external" href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html">official PyTorch DDP tutorial</a>. Additionally, if you are looking for an example of setting up DDP for our cluster, refer to this project: <a class="reference external" href="https://github.com/beenodbaneeya/Distributed-Pytorch/tree/main">DDP Example on Saga</a>.</p>
<section id="key-changes-in-the-code">
<h3>Key Changes in the Code<a class="headerlink" href="#key-changes-in-the-code" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Wrapping the Main Logic</strong>:
The main logic of the code has been encapsulated in a function. This is a common practice when working with DDP to ensure proper initialization and cleanup of distributed processes.</p></li>
<li><p><strong>Data Loading Optimizations</strong>:</p></li>
</ol>
<ul class="simple">
<li><p>We added <code class="docutils literal notranslate"><span class="pre">persistent_workers=True</span></code> to the <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> to improve performance by keeping worker processes alive between iterations.</p></li>
<li><p>We also set <code class="docutils literal notranslate"><span class="pre">non_blocking=True</span></code> for data transfers to overlap data transfer with computation, further optimizing the data loading process.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Memory Efficiency</strong>:<br />
To reduce memory usage, we configured the optimizer with <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad(set_to_none=True)</span></code>. This ensures gradients are set to <code class="docutils literal notranslate"><span class="pre">None</span></code> instead of zeroing them out, which can save memory during training.</p></li>
<li><p><strong>Profiling Across Multiple GPUs</strong>:</p></li>
</ol>
<ul class="simple">
<li><p>Since we are using multiple GPUs, the profiling code now collects metrics from all GPUs. This allows us to analyze the performance of the entire distributed setup.</p></li>
<li><p>However, if you want to profile a specific GPU, you can modify the code to log metrics only for that GPU. For example, by adding a condition like <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">rank</span> <span class="pre">==</span> <span class="pre">0:</span></code>, you can restrict profiling to a single GPU (typically GPU 0).</p></li>
<li><p>While it is possible to profile all GPUs during testing, in practice, profiling is often limited to a single GPU to reduce profiling overhead once the setup is verified.</p></li>
</ul>
</section>
</section>
<section id="job-script-for-utilizing-multiple-gpus">
<h2>Job Script for Utilizing Multiple GPUs<a class="headerlink" href="#job-script-for-utilizing-multiple-gpus" title="Link to this heading"></a></h2>
<p>To run our code on multiple GPUs, we need to make a few modifications to the previous job script. These changes ensure that the script is configured to utilize multiple GPUs effectively and include PyTorch-specific parameters required for distributed training.
For example, we use the <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> utility with the <code class="docutils literal notranslate"><span class="pre">standalone</span></code> argument to indicate that the code will run on multiple GPUs within a single node. This is a key step in enabling PyTorch’s DDP functionality.
Below is the updated job script with the necessary changes for multi-GPU execution.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -l</span>
<span class="c1">#SBATCH --job-name=PyTprofilergpus</span>
<span class="c1">#SBATCH --account=&lt;project_number&gt;</span>
<span class="c1">#SBATCH --time=00:10:00     #wall-time </span>
<span class="c1">#SBATCH --partition=accel   #partition </span>
<span class="c1">#SBATCH --nodes=1           #nbr of nodes</span>
<span class="c1">#SBATCH --ntasks=1          #nbr of tasks</span>
<span class="c1">#SBATCH --ntasks-per-node=1 #nbr of tasks per nodes (nbr of cpu-cores, MPI-processes)</span>
<span class="c1">#SBATCH --cpus-per-task=1   #nbr of threads</span>
<span class="c1">#SBATCH --gpus=2            #total nbr of gpus</span>
<span class="c1">#SBATCH --mem=4G            #main memory</span>
<span class="c1">#SBATCH -o PyTprofilergpus.out  #slurm output </span>

<span class="c1"># Set up job environment</span>
<span class="nb">set</span><span class="w"> </span>-o<span class="w"> </span>errexit<span class="w"> </span><span class="c1"># exit on any error</span>
<span class="nb">set</span><span class="w"> </span>-o<span class="w"> </span>nounset<span class="w"> </span><span class="c1"># treat unset variables as error</span>

<span class="c1">#define paths</span>
<span class="nv">Mydir</span><span class="o">=</span>/cluster/work/users/&lt;user_name&gt;
<span class="nv">MyContainer</span><span class="o">=</span><span class="si">${</span><span class="nv">Mydir</span><span class="si">}</span>/Container/pytorch_22.12-py3.sif
<span class="nv">MyExp</span><span class="o">=</span><span class="si">${</span><span class="nv">Mydir</span><span class="si">}</span>/MyEx

<span class="c1">#specify bind paths by setting the environment variable</span>
<span class="c1">#export SINGULARITY_BIND=&quot;${MyExp},$PWD&quot;</span>

<span class="c1">#TF32 is enabled by default in the NVIDIA NGC TensorFlow and PyTorch containers </span>
<span class="c1">#To disable TF32 set the environment variable to 0</span>
<span class="c1">#export NVIDIA_TF32_OVERRIDE=0</span>

<span class="c1">#to run singularity container </span>
srun<span class="w"> </span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>-B<span class="w"> </span><span class="si">${</span><span class="nv">MyExp</span><span class="si">}</span>,<span class="nv">$PWD</span><span class="w"> </span><span class="si">${</span><span class="nv">MyContainer</span><span class="si">}</span><span class="w"> </span>torchrun<span class="w"> </span>--standalone<span class="w"> </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="si">${</span><span class="nv">SLURM_GPUS_PER_NODE</span><span class="k">:-</span><span class="nv">2</span><span class="si">}</span><span class="w"> </span><span class="si">${</span><span class="nv">MyExp</span><span class="si">}</span>/resnet18_api_ddp.py

<span class="nb">echo</span><span class="w"> </span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;--Job ID:&quot;</span><span class="w"> </span><span class="nv">$SLURM_JOB_ID</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;--total nbr of gpus&quot;</span><span class="w"> </span><span class="nv">$SLURM_GPUS</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;--nbr of gpus_per_node&quot;</span><span class="w"> </span><span class="nv">$SLURM_GPUS_PER_NODE</span>
</pre></div>
</div>
</section>
</section>
<section id="performance-metrics">
<h1>Performance Metrics<a class="headerlink" href="#performance-metrics" title="Link to this heading"></a></h1>
<p>In this section, we present screenshots of various performance metrics captured using the PyTorch Profiler. These metrics provide insights into GPU-specific logs and help us analyze the performance of our multi-GPU setup.</p>
<section id="gpu-usage">
<h2>1. GPU Usage<a class="headerlink" href="#gpu-usage" title="Link to this heading"></a></h2>
<p>When profiling performance metrics for multiple GPUs, the TensorBoard dashboard allows us to select and view the metrics for each GPU individually. For example, as shown in the figure below, we can analyze the GPU utilization for each GPU in the system:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/gpusoverview.png"><img alt="Overview of Multiple GPUs" src="../../_images/gpusoverview.png" style="width: 100%;" />
</a>
</figure>
<p>The dashboard provides detailed information, including GPU utilization for each GPU. In our case, both GPUs are utilized at approximately <strong>33%</strong>, and the profiler also provides performance recommendations for both GPUs. These insights are valuable for identifying bottlenecks and optimizing GPU usage in a distributed training setup.</p>
</section>
<section id="trace-view">
<h2>2. Trace View<a class="headerlink" href="#trace-view" title="Link to this heading"></a></h2>
<p>Similar to the single GPU case, we can now view the trace for each individual GPU in a multi-GPU setup. The trace view provides a detailed timeline of operations, helping us identify potential bottlenecks for each GPU.
As shown in the figure below, the trace view allows us to analyze the execution patterns and performance of each GPU:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/traceoverview.png"><img alt="Trace View for Multiple GPUs" src="../../_images/traceoverview.png" style="width: 100%;" />
</a>
</figure>
<p>This visualization is particularly useful for pinpointing inefficiencies and understanding how workloads are distributed across GPUs in a distributed training setup.</p>
</section>
<section id="memory-view">
<h2>3. Memory View<a class="headerlink" href="#memory-view" title="Link to this heading"></a></h2>
<p>The <strong>Memory View</strong> allows us to compare the memory usage of each GPU over time. This view provides valuable insights into how memory is allocated and utilized by each GPU during training.
By analyzing the memory usage across different timeframes for individual GPUs, we can identify potential inefficiencies and determine whether any optimizations are needed to improve memory utilization.
Below are the memory views for the two GPUs used in our setup:</p>
<ul class="simple">
<li><p><strong>Memory View for GPU 0</strong>:</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/gpu0memoryview.png"><img alt="Memory View GPU 0" src="../../_images/gpu0memoryview.png" style="width: 100%;" />
</a>
</figure>
<ul class="simple">
<li><p><strong>Memory View for GPU 1</strong>:</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/gpu1memoryview.png"><img alt="Memory View GPU 1" src="../../_images/gpu1memoryview.png" style="width: 100%;" />
</a>
</figure>
<p>These visualizations help us monitor and compare memory usage across GPUs, making it easier to identify imbalances or areas for improvement.</p>
</section>
<section id="distributed-view">
<h2>4. Distributed View<a class="headerlink" href="#distributed-view" title="Link to this heading"></a></h2>
<p>The <strong>Distributed View</strong> provides detailed information about the devices used in the multi-GPU setup. This includes details such as the device name, memory usage, and other relevant metrics for each GPU.
As shown in the figure below, this view helps us understand the hardware configuration and resource utilization for each GPU:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../_images/distributedview.png"><img alt="Distributed View" src="../../_images/distributedview.png" style="width: 100%;" />
</a>
</figure>
<p>Additionally, the <strong>Distributed View</strong> offers an overview of computation and synchronization across GPUs in a graphical format. This visualization is particularly useful for analyzing how workloads are distributed and synchronized between GPUs, helping us identify potential inefficiencies in the distributed training.</p>
</section>
</section>
<section id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading"></a></h1>
<p>In this guide, we demonstrated how to profile GPU-accelerated deep learning models using the PyTorch Profiler in a multi-GPU setup. By leveraging multiple GPUs, we showcased how to analyze performance metrics, identify bottlenecks, and optimize resource utilization to improve the efficiency of distributed training workflows.</p>
</section>
<section id="relevant-links">
<h1>Relevant links<a class="headerlink" href="#relevant-links" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a></p>
<p><a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch">NVIDIA NGC container</a></p>
<p><a class="reference external" href="https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html">official PyTorch DDP tutorial</a></p>
<p><a class="reference external" href="https://github.com/beenodbaneeya/Distributed-Pytorch/tree/main">DDP Example on Saga</a></p>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../guides_python.html" class="btn btn-neutral float-left" title="Python libraries" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pytorch_olivia/PyTorchSingleGpu.html" class="btn btn-neutral float-right" title="PyTorch on Olivia" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, Sigma2/NRIS. Text shared under CC-BY 4.0 license.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>