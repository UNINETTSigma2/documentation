

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Short Instructions Video Archives &mdash; Sigma2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/nris.css?v=69e7a171" />
      <link rel="stylesheet" type="text/css" href="../_static/universal-navbar.css" />
      <link rel="stylesheet" type="text/css" href="../_static/statuspal.css" />

  
    <link rel="shortcut icon" href="../_static/nris.ico"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=f281be69"></script>
      <script async="async" src="https://siteimproveanalytics.com/js/siteanalyze_6036825.js"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../_static/statuspal_widget.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training materials" href="material.html" />
    <link rel="prev" title="Training Video Archives" href="videos.html" /> 
</head>

<body class="wy-body-for-nav">
<!-- Send url to parent when displayed as iframe -->
<script>
    const valid_orign_url = "https://www.sigma2.no"
    window.addEventListener('message', function(event) {
        if (event.data === 'getDocumentationIframeUrl' && event.origin.startsWith(valid_orign_url)) {
            // path only (/path/example.html)
            const path = window.location.pathname
            // query string (including the initial ? symbol)
            const search = window.location.search
            // Returns the hash (including the initial # symbol)
            const hash = window.location.hash
            const newUrl = path + search + hash;
            event.source.postMessage(newUrl, event.origin)
        }
    })

</script>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Sigma2/NRIS documentation
              <img src="../_static/NRIS Logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Policies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code-of-conduct.html">Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/acceptable-use-policy">User Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/security-policy.html">Security policy for Sigma2 infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/sharing_files.html">Data handling and storage policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/licenses.html">Licence and access policies</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/data-policy">Data Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/data-decommissioning-policies">Data decommissioning policies</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/central-data-library-policy">Central Data Library Policy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/policies">Overview of Sigma2 Policies</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/support_line.html">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/extended_support.html">Extended support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/how_to_write_good_support_requests.html">Writing good support requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/qa-sessions.html">Open Question &amp; Answer Sessions for All Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/lost_forgotten_password.html">Lost, expiring or changing passwords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/two_factor_authentication.html">One-time-pad (OTP) / Two-factor authentication</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/project-leader-handbook">Project Leader Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="events.html">Training events</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes_qa.html">Questions, Answers and Feedbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="videos.html">Training Video Archives</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Short Instructions Video Archives</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#parallel-computing">Parallel Computing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-basics-on-the-saga-linux-cluster">1.The Basics on the Saga Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interactive-jobs-on-the-saga-compute-nodes">2.Interactive Jobs on the Saga compute nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cpu-billing-on-the-saga-compute-nodes">3.CPU billing on the Saga compute nodes.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fortran-formula-translation">4.Fortran = Formula Translation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fortran-compiling-using-the-intel-fortran-compiler">5.Fortran Compiling using the Intel Fortran compiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fortran-optimization">6.Fortran Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-offloading-on-the-saga-linux-cluster">7.GPU Offloading on the Saga Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-offloading-testing-on-the-saga-linux-cluster">8.GPU Offloading testing on the Saga Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openacc-offloading-on-the-p-100-nvidia-gpus">9.OpenACC offloading on the P-100 Nvidia GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openacc-offloading-on-the-a-100-nvidia-gpus">10.OpenACC offloading on the A-100 Nvidia GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-open-multi-processing-and-the-fram-linux-cluster">11.OpenMP (Open Multi-Processing) and the Fram Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-on-the-fram-linux-cluster">12.OpenMP on the Fram Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-on-the-betzy-linux-cluster">13.OpenMP on the Betzy Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-affinity-on-the-fram-linux-cluster">14.OpenMP affinity on the Fram Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-affinity-on-the-betzy-linux-cluster">15.OpenMP affinity on the Betzy Linux cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp-summary">16.OpenMP Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#typing-versus-scripting">17.Typing versus Scripting.</a></li>
<li class="toctree-l3"><a class="reference internal" href="#open-mp-offloading-on-the-a-100-nvidia-gpus">18.Open-MP offloading on the A-100 Nvidia GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-gpu-offloading-on-the-a-100-nvidia-gpus">19.Multi-GPU offloading on the A-100 Nvidia GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus-error-testing">20.Multi-GPU parallel computing on the A-100 Nvidia GPUs - Error testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#more-multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus">21.More multi-GPU parallel computing on the A-100 Nvidia GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus">22.Multi-GPU parallel computing on the A-100 Nvidia GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="#open-mp-with-open-acc-offloading-for-multi-gpu-parallel-computing">23.Open-MP with Open-ACC offloading for multi-GPU parallel computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">24.Open-MP with Open-ACC offloading for multi-GPU parallel computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cuda-fortran">25.CUDA Fortran</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-summary">26.GPU Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduling-jobs-from-the-home-folder">27.Scheduling Jobs from the Home folder</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scheduling-jobs-from-the-work-folder">28.Scheduling Jobs from the Work folder</a></li>
<li class="toctree-l3"><a class="reference internal" href="#jobs-on-fram">29.Jobs on Fram</a></li>
<li class="toctree-l3"><a class="reference internal" href="#more-jobs-on-fram">30.More Jobs on Fram</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mkl-on-fram">31.MKL on Fram</a></li>
<li class="toctree-l3"><a class="reference internal" href="#job-efficiency-on-fram">32.Job Efficiency on Fram</a></li>
<li class="toctree-l3"><a class="reference internal" href="#jobs-on-betzy">33.Jobs on Betzy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mkl-on-betzy">34.MKL on Betzy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#job-efficiency-on-betzy">35.Job Efficiency on Betzy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fortran-and-python">36.Fortran and Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#python-on-fram">37.Python on Fram</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mkl-with-python-on-fram">38.MKL with Python on Fram</a></li>
<li class="toctree-l3"><a class="reference internal" href="#python-on-betzy">39.Python on Betzy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mkl-with-python-on-betzy">40.MKL with Python on Betzy</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cray-openmp-on-olivia">41.Cray OpenMP on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#more-cray-openmp-on-olivia">42.More Cray OpenMP on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cray-mkl-on-olivia">43.Cray MKL on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cray-mkl-scaling-on-olivia">44.Cray MKL Scaling on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nris-intel-openmp-scaling-on-olivia">45.NRIS Intel OpenMP Scaling on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nris-intel-mkl-scaling-on-olivia">46.NRIS Intel MKL Scaling on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cray-intel-openmp-scaling-on-olivia">47.Cray Intel OpenMP Scaling on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cray-intel-mkl-scaling-on-olivia">48.Cray Intel MKL Scaling on Olivia</a></li>
<li class="toctree-l3"><a class="reference internal" href="#summary-of-scaling-on-olivia">49.Summary of Scaling on Olivia</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#other-topics">Other topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="material.html">Training materials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/opslog.html">Status and maintenance of systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/applying_account.html">How do I get an account?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/applying_resources.html">Applying for computing and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/file_transfer.html">File transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/editing_files.html">Editing files</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_development/guides/vs_code/connect_to_server.html">Connecting to a system with Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/ssh.html">SSH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/ssh.html#common-ssh-errors">Common SSH errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/ood.html">Open OnDemand</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/R.html">First R calculation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data and Storage Services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird/nird_dp.html">NIRD Data Peak</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird/nird_dl.html">NIRD Data Lake</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird/backup_lmd.html">NIRD Backup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird/cdl.html">(NIRD) Central Data Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nird_archive/user-guide.html">NIRD Research Data Archive (NIRD RDA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nird_service_platform/overview_nird_service_platform.html">NIRD Service Platform</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Storage Resources and Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird_lmd.html">NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/clusters.html">Storage areas on HPC clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/quota.html">Storage quota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/backup.html">Backup on Betzy, Saga, and NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/performance.html">Optimizing storage performance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HPC usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/migration2metacenter.html">Migration to an NRIS HPC machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computing/responsible-use.html">Using shared resources responsibly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jobs/overview.html">Running jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jobs/internet-login-compute-nodes.html">Login nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jobs/internet-login-compute-nodes.html#compute-nodes">Compute nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computing/tuning-applications.html">Tuning applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_development/guides_llm.html">Running LLM Models in a Cluster Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compute resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/hardware_overview.html">Overview over our machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/betzy.html">Betzy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/olivia.html">Olivia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/saga.html">Saga</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/lumi.html">LUMI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../software/modulescheme.html">Software module scheme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/installed_software.html">Installed software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/userinstallsw.html">Installing software as user</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/appguides.html">Application guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/eessi.html">EESSI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tools and Additional services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../nird_toolkit/overview.html">NIRD Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/course_resources.html">CRaaS - Course Resources as a Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code development and tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code_development/overview.html">Code development and tutorials</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Sigma2/NRIS documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Short Instructions Video Archives</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="short-instructions-video-archives">
<span id="short-videos"></span><h1>Short Instructions Video Archives<a class="headerlink" href="#short-instructions-video-archives" title="Link to this heading"></a></h1>
<p>Here we share the link to our archive of short instruction videos, made available on the <a class="reference external" href="https://www.youtube.com/channel/UCG6fTXEY_SQYohtpU6aZwPw">NRIS YouTube channel</a>:</p>
<section id="parallel-computing">
<h2>Parallel Computing<a class="headerlink" href="#parallel-computing" title="Link to this heading"></a></h2>
<p><a class="reference external" href="https://www.youtube.com/playlist?list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O">In this video playlist</a>, we will cover parallel computing on Linux clusters. Parallel computing involves performing multiple calculations or processes at the same time to break down a large task into smaller, manageable sub-tasks. This approach typically boosts performance and efficiency.</p>
<p>We will start by using Fortran for parallel computing to perform matrix multiplication. Fortran is one of the oldest programming languages still widely used today, especially in high-performance computing. As we continue with these videos, we will also explore parallel computing using Python and C.</p>
<p>In these videos, we emphasize the use of Python scripts. While creating these scripts takes time, they can be reused, enhancing efficiency and serving as useful documentation of your processes. Initially, typing Linux commands directly might seem quicker, but using scripts becomes a time-saver as you advance. Regularly entering the same Linux commands without scripts is inefficient.</p>
<p>In these videos, we will be using the Emacs editor, but please feel free to use any editor that you are comfortable with. We hope these videos will enhance your parallel computing skills. Enjoy your computing journey!</p>
<p>If anything was unclear or you think something should have been explained in more detail, please let us know in the video comments on Youtube. We appreciate your feedback and it will help us improving these videos.</p>
<p>To use the scripts and files from the videos as templates for your work, copy them from this folder on the Saga Linux Cluster: /cluster/work/support/ParallelComputingYoutube</p>
<section id="the-basics-on-the-saga-linux-cluster">
<h3>1.The Basics on the Saga Linux cluster<a class="headerlink" href="#the-basics-on-the-saga-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=LSoRhTMPeWk&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=1">In this video</a>, we explore the Saga Linux cluster in detail. We demonstrate basic Linux commands and how to determine your location within the cluster. Additionally, we review the various types of compute nodes available on the Saga Linux cluster.</p>
</section>
<section id="interactive-jobs-on-the-saga-compute-nodes">
<h3>2.Interactive Jobs on the Saga compute nodes<a class="headerlink" href="#interactive-jobs-on-the-saga-compute-nodes" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=tEpbr6fKjIs&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=2">In this video</a>, we demonstrate how to launch an interactive job on the compute nodes. We walk you through a Python setup script that employs the Slurm-allocate command, explaining different options to move from the login nodes to one of the compute nodes.</p>
</section>
<section id="cpu-billing-on-the-saga-compute-nodes">
<h3>3.CPU billing on the Saga compute nodes.<a class="headerlink" href="#cpu-billing-on-the-saga-compute-nodes" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=rtALtvMsPoM&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=3">In this video</a>, we explain how to determine the billing factor for various jobs on the compute nodes. We show you how to create a Linux alias for the CPU billing factor and demonstrate its usage. Additionally, we discuss how CPU time varies based on the job’s memory and CPU core allocations.</p>
</section>
<section id="fortran-formula-translation">
<h3>4.Fortran = Formula Translation<a class="headerlink" href="#fortran-formula-translation" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=SWwiegpzVXw&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=4">In this video</a>, we cover key aspects of the Fortran programming language, highlighting its use in matrix multiplication. We outline a Fortran program tailored for this task and introduce a Python testing script that aids in compiling and evaluating the program.
The use of a testing script is vital for maintaining consistent performance benchmarks and provides a documented method of how testing was executed.</p>
</section>
<section id="fortran-compiling-using-the-intel-fortran-compiler">
<h3>5.Fortran Compiling using the Intel Fortran compiler<a class="headerlink" href="#fortran-compiling-using-the-intel-fortran-compiler" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=0r54WVnB-KU&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=5">In this video</a>, we explore essential features of the Intel Fortran compiler, emphasizing optimization options for the Intel Sky-lake AVX-512 architecture.
We examine different techniques for calculating matrix multiplication through accumulative summation. We also guide you on how to measure the memory usage of your Fortran programs.</p>
</section>
<section id="fortran-optimization">
<h3>6.Fortran Optimization<a class="headerlink" href="#fortran-optimization" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=GnVD2eZIvjs&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=6">In this video</a>, we investigate the optimization capabilities of the Intel Fortran compiler, focusing on matrix multiplication performance in a parallel computing setting on a single CPU core.
We’ll review the compiler’s optimization reports, explore loop vectorization, and discuss various strategies to enhance matrix multiplication efficiency. Furthermore, we’ll provide an estimation of the maximum theoretical performance on the Sky-lake architecture.</p>
</section>
<section id="gpu-offloading-on-the-saga-linux-cluster">
<h3>7.GPU Offloading on the Saga Linux cluster<a class="headerlink" href="#gpu-offloading-on-the-saga-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=_EgU49Mbm90&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=7">In this video</a>, we explore GPU offloading for a Fortran program designed for matrix multiplication. We employ a Fortran program that utilizes the do concurrent method for GPU offloading.
Additionally, we introduce a Python testing script specifically for GPU offloading scenarios. We’ll guide you through the necessary compilation options for offloading and show you how to monitor GPU usage.</p>
</section>
<section id="gpu-offloading-testing-on-the-saga-linux-cluster">
<h3>8.GPU Offloading testing on the Saga Linux cluster<a class="headerlink" href="#gpu-offloading-testing-on-the-saga-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=vuHVjlZu5Hg&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=8">In this video</a>, we examine GPU offloading techniques using a Fortran program tailored for matrix multiplication. We demonstrate the use of the do concurrent method in a Fortran program to enable GPU offloading.
Alongside this, we employ a Python testing script designed for GPU offloading scenarios. We assess the performance differences caused by various loop orderings in the matrix multiplication loop. Our testing includes performance evaluations on both the Nvidia P-100 and A-100 GPUs.
We also summarize the performance testing of the Fortran program for matrix multiplication that we’ve conducted in our recent videos, focusing on both a single CPU core and a single GPU.</p>
</section>
<section id="openacc-offloading-on-the-p-100-nvidia-gpus">
<h3>9.OpenACC offloading on the P-100 Nvidia GPUs<a class="headerlink" href="#openacc-offloading-on-the-p-100-nvidia-gpus" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=FLQM-9vUzPA&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=9">In this video</a>, we evaluate the performance of a single GPU on the P-100 nodes, specifically focusing on its efficiency in matrix multiplication, measured in Giga-Flops per second.
We utilize OpenACC directives to facilitate this process. We’ll employ an OpenACC environmental variable to monitor GPU performance and use the optimization report feature of the Nvidia Fortran compiler to examine the compilation optimizations. Additionally, we explore the effects of different loop orderings in the matrix multiplication loop.</p>
</section>
<section id="openacc-offloading-on-the-a-100-nvidia-gpus">
<h3>10.OpenACC offloading on the A-100 Nvidia GPUs<a class="headerlink" href="#openacc-offloading-on-the-a-100-nvidia-gpus" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=mdlWUqp1SwU&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=10">In this video</a>, we assess the performance of a single GPU on the A-100 nodes, with a particular focus on matrix multiplication efficiency. We use OpenACC directives to streamline this process and conduct the testing as a batch job.
We’ll utilize an OpenACC environmental variable to track GPU performance and explore the optimization report feature of the Nvidia Fortran compiler. Additionally, we investigate the impact of varying the size of loop gang vectors in the matrix multiplication loop.</p>
</section>
<section id="openmp-open-multi-processing-and-the-fram-linux-cluster">
<h3>11.OpenMP (Open Multi-Processing) and the Fram Linux cluster<a class="headerlink" href="#openmp-open-multi-processing-and-the-fram-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=5xlvfsGV6-M&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=11">In this video</a>, we explore parallel computing on CPU cores, specifically focusing on OpenMP.
We begin by assessing the performance of a Fortran program designed for matrix multiplication on a single CPU core on the Fram Linux cluster.
We also provide an overview of a Fortran program and a Python testing script that we use for parallel computing with OpenMP on the Fram Linux cluster.</p>
</section>
<section id="openmp-on-the-fram-linux-cluster">
<h3>12.OpenMP on the Fram Linux cluster<a class="headerlink" href="#openmp-on-the-fram-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=vp33zlzBnWA&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=12">In this video</a>, we focus on parallel computing using OpenMP (Open Multi-Processing) on the Fram Linux cluster. We analyze the performance by experimenting with different numbers of threads.
To clearly demonstrate the impact of thread count on performance, we will present graphs of both wall time and CPU time, highlighting their dependency on the number of threads used.</p>
</section>
<section id="openmp-on-the-betzy-linux-cluster">
<h3>13.OpenMP on the Betzy Linux cluster<a class="headerlink" href="#openmp-on-the-betzy-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=tlUw7vQm0X0&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=13">In this video</a>, we explore parallel computing with OpenMP (Open Multi-Processing) on the Betzy Linux cluster. We experiment with varying numbers of threads to analyze performance differences on the Betzy Linux cluster.
To visually demonstrate how thread count affects performance, we will show graphs of both wall time and CPU time, emphasizing their correlation with the number of threads.</p>
</section>
<section id="openmp-affinity-on-the-fram-linux-cluster">
<h3>14.OpenMP affinity on the Fram Linux cluster<a class="headerlink" href="#openmp-affinity-on-the-fram-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=GFFCYmbwiH4&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=14">In this video</a>, we investigate the role of thread affinity in enhancing OpenMP performance on the Fram Linux cluster.
We experiment with different numbers of threads and affinity configurations to analyze their effects on performance.
To clearly demonstrate these effects, we will show graphs of both wall time and CPU time, emphasizing how performance varies with changes in thread count.</p>
</section>
<section id="openmp-affinity-on-the-betzy-linux-cluster">
<h3>15.OpenMP affinity on the Betzy Linux cluster<a class="headerlink" href="#openmp-affinity-on-the-betzy-linux-cluster" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=fGFkVRC1Yx4&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=15">In this video</a>, we explore the impact of thread affinity on OpenMP performance on the Betzy Linux cluster. We conduct experiments using various numbers of threads and different affinity settings to assess their influence on performance.
To clearly illustrate these impacts, we will present graphs of both wall time and CPU time, highlighting the variations in performance as the thread count changes.</p>
</section>
<section id="openmp-summary">
<h3>16.OpenMP Summary<a class="headerlink" href="#openmp-summary" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=MZ1LPnSRuKk&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=16">In this video</a>, we summarize the performance outcomes of using OpenMP (Open Multi-Processing) for parallel computing on the Betzy and Fram Linux clusters.
We’ll compare wall and CPU times for various methods showcased in our previous videos. Additionally, we’ll discuss why using a large number of threads can lead to bottlenecks in parallel computing with multi-threading, and how transitioning from multi-threading to multi-tasking can address this issue.</p>
</section>
<section id="typing-versus-scripting">
<h3>17.Typing versus Scripting.<a class="headerlink" href="#typing-versus-scripting" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=OWj0Vbvsbc0&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=17">In this video</a>, we’ll show you how to speed up typing Linux commands with the autocomplete feature. We’ll also discuss the advantages of using scripts over manual typing.
While scripts take time to create, they can be reused multiple times. Scripts are also serving as a detailed record of your procedures. This is especially useful in performance testing, where consistent methods are essential for accurate comparison of different parallel computing strategies.
Beyond performance testing, Python is excellent for automating various routine tasks. Initially, manual typing may appear quicker, but as you’ll experience, scripting ultimately saves time and reduces repetitive work.</p>
</section>
<section id="open-mp-offloading-on-the-a-100-nvidia-gpus">
<h3>18.Open-MP offloading on the A-100 Nvidia GPUs<a class="headerlink" href="#open-mp-offloading-on-the-a-100-nvidia-gpus" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=omrx1IJesVc&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=18">In this video</a>, we evaluate the performance of a single GPU on the A-100 nodes by utilizing Open-MP directives. We perform our tests as batch jobs from the work folder on the Saga Linux cluster.
We present a Fortran program designed for GPU offloading with Open-MP. Additionally, we examine how different loop orders in the matrix multiplication process affect performance.</p>
</section>
<section id="multi-gpu-offloading-on-the-a-100-nvidia-gpus">
<h3>19.Multi-GPU offloading on the A-100 Nvidia GPUs<a class="headerlink" href="#multi-gpu-offloading-on-the-a-100-nvidia-gpus" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=tGm-CtR_TiY&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=19">In this video</a>, we assess the performance of multi-GPU parallel computing on A-100 nodes using Open-MP directives. We conduct our tests as batch jobs from the work folder on the Saga Linux cluster. We also introduce a Fortran program tailored for multi-GPU offloading with Open-MP.</p>
</section>
<section id="multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus-error-testing">
<h3>20.Multi-GPU parallel computing on the A-100 Nvidia GPUs - Error testing<a class="headerlink" href="#multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus-error-testing" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=waQKXpt6WCw&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=20">In this video</a>, we offer a detailed demonstration of using a Fortran program to verify the accuracy of our multi-GPU parallel computing approach for matrix multiplication. We’ll walk you through a process to identify any errors in this method.
Additionally, we show how to use allocatable arrays in Fortran, which are dynamically allocated at runtime instead of being statically allocated at compile time.</p>
</section>
<section id="more-multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus">
<h3>21.More multi-GPU parallel computing on the A-100 Nvidia GPUs<a class="headerlink" href="#more-multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus" title="Link to this heading"></a></h3>
<p>In our previous video, we identified an issue with our multi-GPU parallel computing method. <a class="reference external" href="https://www.youtube.com/watch?v=ZDaBgBNQcUk&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=21">In this video</a>, we will tackle this problem and introduce a new approach for multi-GPU parallel computing. We’ll also present a new Fortran program that utilizes Open-MP for multi-GPU computing.</p>
</section>
<section id="multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus">
<h3>22.Multi-GPU parallel computing on the A-100 Nvidia GPUs<a class="headerlink" href="#multi-gpu-parallel-computing-on-the-a-100-nvidia-gpus" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=kJkW2LxsX10&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=22">In this video</a>, we’ll demonstrate how to test our new multi-GPU parallel computing method. We will use up to 4 GPUs on a single node and conduct these tests as batch jobs from the work folder on the Saga Linux cluster. Additionally, we’ll guide you through a process to detect any errors in this new multi-GPU parallel computing approach.</p>
</section>
<section id="open-mp-with-open-acc-offloading-for-multi-gpu-parallel-computing">
<h3>23.Open-MP with Open-ACC offloading for multi-GPU parallel computing<a class="headerlink" href="#open-mp-with-open-acc-offloading-for-multi-gpu-parallel-computing" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=xSxpfPKFcEY&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=23">In this video</a>, we explore multi-GPU parallel computing by combining Open-MP with Open-ACC. We introduce a Fortran program designed for multi-GPU parallel computing that utilizes both Open-MP and Open-ACC for offloading.
We will use two GPUs on a single node and conduct our tests as batch jobs from the work folder on the Saga Linux cluster.</p>
</section>
<section id="id1">
<h3>24.Open-MP with Open-ACC offloading for multi-GPU parallel computing<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=3Lsn7-hsd0s&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=24">In this video</a>, we will address the issue highlighted in our previous video. To resolve it, we need to use the data directive when offloading with Open-ACC, especially important in multi-GPU parallel computing where the compiler needs extra guidance. Simply using the kernels directive, as we did with a single GPU, is not enough for multi-GPU setups.
We will use up to 4 GPUs on a single node and conduct these tests as batch jobs from the work folder on the Saga Linux cluster.</p>
</section>
<section id="cuda-fortran">
<h3>25.CUDA Fortran<a class="headerlink" href="#cuda-fortran" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=zuKWhh0mSZE&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=25">In this video</a>, we’ll focus on GPU offloading through CUDA Fortran. CUDA, created by NVIDIA, is a parallel computing platform and API that allows for programming directly on GPUs.
CUDA Fortran is an enhancement to the Fortran programming language, enabling it to utilize the features of CUDA directly.
Previously, we mainly used directive-based GPU offloading with Open-ACC and Open-MP.
Unlike these approaches, CUDA is a kernel-based offloading, that involves explicitly writing kernels to manage offloading.</p>
</section>
<section id="gpu-summary">
<h3>26.GPU Summary<a class="headerlink" href="#gpu-summary" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=Su2LJB6Yl0s&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=26">In this video</a>, we’ll summarize the GPU offloading methods we’ve covered so far.
In our first video with GPU offloading, we explored language-based offloading using the do concurrent construct available in Fortran.
Following that, we primarily focused on directive-based GPU offloading with OpenACC and OpenMP.
In our most recent video, we used kernel-based offloading with CUDA Fortran, which involves explicitly writing kernels to handle offloading.</p>
</section>
<section id="scheduling-jobs-from-the-home-folder">
<h3>27.Scheduling Jobs from the Home folder<a class="headerlink" href="#scheduling-jobs-from-the-home-folder" title="Link to this heading"></a></h3>
<p>In earlier videos, we have used job scripts for parallel computing. <a class="reference external" href="https://www.youtube.com/watch?v=6ExMu8ee3sE&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=27">In this video</a>, we’ll examine more details of scheduling jobs on the Fram Linux cluster with the Slurm management system.
Please note that we will revisit some topics we’ve covered before, so there will be some repetition.
The purpose of Slurm (Simple Linux Utility for Resource Management) is to provide a robust, scalable, and efficient way to manage and schedule jobs on clusters in high-performance computing (HPC) environments.</p>
</section>
<section id="scheduling-jobs-from-the-work-folder">
<h3>28.Scheduling Jobs from the Work folder<a class="headerlink" href="#scheduling-jobs-from-the-work-folder" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=cjqgQXTOhmI&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=28">In this video</a>, we’ll continue to explore details about scheduling jobs on the Fram Linux cluster with the slurm management system. More specifically, we will provide more details about submitting jobs from the work folder.</p>
</section>
<section id="jobs-on-fram">
<h3>29.Jobs on Fram<a class="headerlink" href="#jobs-on-fram" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=MjgDxKUxWsY&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=29">In this video</a>, we will provide more details on executing jobs on Fram Linux cluster.
In our previous videos, we concentrated on performance testing for different parallel computing methods for matrix multiplication using a Fortran program. During these tests, we utilized some Python testing scripts.
Once the testing phase is complete, you will schedule your computation code as a production job. We will offer some suggestions on how to structure a job script for this purpose.</p>
</section>
<section id="more-jobs-on-fram">
<h3>30.More Jobs on Fram<a class="headerlink" href="#more-jobs-on-fram" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=FpAfNC9fjks&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=30">In this video</a>, we will provide more details on executing jobs on Fram Linux cluster.
We will attempt to increase the matrix dimension to 65536 and execute this matrix multiplication process as a parallel computation on the Fram Linux cluster using OpenMP.
When we used a matrix dimension of 32768, the memory usage was about 24 gigabytes. Therefore, increasing the dimension to 65536 would result in a memory usage of approximately 100 gigabytes.
Thus, we need to use the big memory compute nodes on the Fram Linux cluster.</p>
</section>
<section id="mkl-on-fram">
<h3>31.MKL on Fram<a class="headerlink" href="#mkl-on-fram" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=AZyMgduUt54&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=31">In this video</a>, we will explain how to schedule jobs on the Fram Linux cluster using the Math Kernel Library (MKL) to perform matrix multiplication.
MKL is an optimized library by Intel offering a variety of mathematical functions for scientific computing. It includes: Linear Algebra, Fast Fourier Transforms, Vector Math, Statistics and Data Fitting.
MKL is designed to maximize the performance of Intel processors through parallelism and vectorization, making it ideal for high-performance computing.</p>
</section>
<section id="job-efficiency-on-fram">
<h3>32.Job Efficiency on Fram<a class="headerlink" href="#job-efficiency-on-fram" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=0afG3u0Odg8&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=32">In this video</a>, we will examine the efficiency of jobs on the Fram Linux cluster.
We will look at jobs scheduled from 22th of February 2023, to 21th of September 2024, covering a period of about 17 months. During this time, 900 000 jobs were scheduled on the Fram Linux cluster.
If you’ve made an effort to perform parallel computing but are experiencing low CPU efficiency, you can request <a class="reference external" href="https://documentation.sigma2.no/getting_help/extended_support/eus.html">extended user support (EUS)</a>.
These projects do not require any funding or in-kind contributions from the project or user. The total work effort should not exceed 70-80 hours over a maximum of 3-4 weeks.</p>
</section>
<section id="jobs-on-betzy">
<h3>33.Jobs on Betzy<a class="headerlink" href="#jobs-on-betzy" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=UDLqdJVOtjg&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=33">In this video</a>, we will provide more details on executing jobs on Betzy Linux cluster.
In our previous videos, we concentrated on performance testing for different parallel computing methods for matrix multiplication using a Fortran program. During these tests, we utilized some Python testing scripts.
Once the testing phase is complete, you will schedule your computation code as a production job. We will offer some suggestions on how to structure a job script for this purpose.</p>
</section>
<section id="mkl-on-betzy">
<h3>34.MKL on Betzy<a class="headerlink" href="#mkl-on-betzy" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=EBk_4fgjaK8&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=34">In this video</a>, we will explain how to schedule jobs on the Betzy Linux cluster using the Math Kernel Library (MKL) to perform matrix multiplication.
MKL is an optimized library by Intel offering a variety of mathematical functions for scientific computing. It includes: Linear Algebra, Fast Fourier Transforms, Vector Math, Statistics and Data Fitting.
MKL is designed to maximize the performance of Intel processors through parallelism and vectorization, making it ideal for high-performance computing.</p>
</section>
<section id="job-efficiency-on-betzy">
<h3>35.Job Efficiency on Betzy<a class="headerlink" href="#job-efficiency-on-betzy" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=ZF_zwyv7Qmw&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=35">In this video</a>, we will examine the efficiency of jobs on the Betzy Linux cluster.
We will look at jobs scheduled from 25th of February 2023, to 10th of August 2024, covering a period of about 16 months. During this time, 400 000 jobs were scheduled on the Betzy Linux cluster.
If you’ve made an effort to perform parallel computing but are experiencing low CPU efficiency, you can request <a class="reference external" href="https://documentation.sigma2.no/getting_help/extended_support/eus.html">extended user support (EUS)</a>.
These projects do not require any funding or in-kind contributions from the project or user. The total work effort should not exceed 70-80 hours over a maximum of 3-4 weeks.</p>
</section>
<section id="fortran-and-python">
<h3>36.Fortran and Python<a class="headerlink" href="#fortran-and-python" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=zJbayn6HVvo&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=36">In this video</a>, we will perform matrix multiplication using a Python program and demonstrate integrating Python with Fortran.
We will use a NumPy routine for matrix multiplication, which efficiently multiplies matrices according to linear algebra rules. This routine is about ten thousand times faster than a triple nested loop in Python.
Fortran to Python, a tool within NumPy, connects Python and Fortran by generating Fortran modules, enabling Python to call Fortran codes. This integration is beneficial for leveraging Fortran’s performance in numerical computations.
We will execute a Python program using a Fortran module for initialisation, which is about 100 times faster than without the module.</p>
</section>
<section id="python-on-fram">
<h3>37.Python on Fram<a class="headerlink" href="#python-on-fram" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=wwuGphSlEFA&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=37">In this video</a>, we will perform matrix multiplication using Python and provide insights into executing production jobs on the Fram Linux cluster.
The matrix multiplication finished in about 88 seconds, confirming that NumPy routines can leverage OpenMP for parallel processing.
This was roughly 20 seconds slower than both the Fortran (video 29) and Fortran MKL (video 31) programs on the Fram Linux cluster.</p>
</section>
<section id="mkl-with-python-on-fram">
<h3>38.MKL with Python on Fram<a class="headerlink" href="#mkl-with-python-on-fram" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=w-Zh7e640dI&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=38">In this video</a>, we will use a Python program with the Fortran MKL library on the Fram Linux cluster.
The matrix multiplication took about 92 seconds with MKL, compared to 88 seconds with NumPy (video 37), showing similar performance.
This confirms that Fortran MKL routines can be effectively used with Python. However, it is advisable to first check if your calculation can be performed with NumPy before considering Fortran MKL.</p>
</section>
<section id="python-on-betzy">
<h3>39.Python on Betzy<a class="headerlink" href="#python-on-betzy" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=P8temHvvcqk&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=39">In this video</a>, we will perform matrix multiplication using Python and provide details on executing production jobs on the Betzy Linux cluster.
The matrix multiplication finished in about 35 seconds, confirming that NumPy routines can leverage OpenMP for parallel processing.
Compared to previous videos, the NumPy routine’s performance is between the 39 seconds with the Fortran program (video 33) and the 31 seconds with the Fortran MKL routine (video 34).</p>
</section>
<section id="mkl-with-python-on-betzy">
<h3>40.MKL with Python on Betzy<a class="headerlink" href="#mkl-with-python-on-betzy" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=jVZdyW1kH6I&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=40">In this video</a>, we will use a Python program with the Fortran MKL library on the Betzy Linux cluster.
The matrix multiplication took about 82 seconds with the Fortran MKL routine, whereas the NumPy routine completed it in 35 and 37 seconds in our last video. This shows that NumPy offers better performance on Betzy.
It is wise to check if your calculation can be done with NumPy before considering Fortran MKL. Both libraries generally outperform independent efforts, so exploring high-performance libraries like NumPy and Fortran MKL can be very beneficial.</p>
</section>
<section id="cray-openmp-on-olivia">
<h3>41.Cray OpenMP on Olivia<a class="headerlink" href="#cray-openmp-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=TR8M2Mzk780&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=41">In this video</a>, we will explore matrix multiplication using a triple nested loop with OpenMP for parallel computing, utilizing the Cray Fortran compiler loaded through the Cray module system on the Olivia Linux cluster.
We will also demonstrate how to submit jobs to the Slurm queue on Olivia and show how to use the Cray module system on Olivia, both in your home environment and within your Slurm job scripts.</p>
</section>
<section id="more-cray-openmp-on-olivia">
<h3>42.More Cray OpenMP on Olivia<a class="headerlink" href="#more-cray-openmp-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=CVasTSdD-CQ&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=42">In this video</a>, we will perform matrix multiplication using a triple nested loop with OpenMP for parallel computing, utilizing the Cray Fortran compiler loaded through the Cray module system on the Olivia Linux cluster.
Additionally, we will demonstrate how to use allocatable matrices in a Fortran program and how modifying the loop ordering of a Fortran do-loop can enhance the program’s performance.</p>
</section>
<section id="cray-mkl-on-olivia">
<h3>43.Cray MKL on Olivia<a class="headerlink" href="#cray-mkl-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=N_9ecZw9paw&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=43">In this video</a>, we will use the MKL routine for matrix multiplication with OpenMP for parallel computing, utilizing the Fortran compiler loaded through the Cray module system.
We will demonstrate that using MKL results in a significant performance improvement compared to the results achieved in video 42.
The Math Kernel Library delivers outstanding performance on the Olivia Linux cluster when used with the Cray module system, particularly when compared to its performance on Betzy and Fram.</p>
</section>
<section id="cray-mkl-scaling-on-olivia">
<h3>44.Cray MKL Scaling on Olivia<a class="headerlink" href="#cray-mkl-scaling-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=afF6ujKZIQ0&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=44">In this video</a>, we will use the MKL routine for matrix multiplication with OpenMP for parallel computing, utilizing the Fortran compiler loaded through the Cray module system.
Additionally, we will conduct a scaling analysis of the Fortran program using MKL with OpenMP on the Olivia Linux cluster.
This involves measuring computation times with varying numbers of threads, starting from a single thread and scaling up to 256 threads.
Scaling analysis in parallel computing is a method used to evaluate and understand how efficiently a parallel system performs as the number of processing units increases.
It helps determine whether adding more computational resources improves performance and by how much. Scaling analysis is crucial for optimizing parallel systems and ensuring that resources are used effectively.</p>
</section>
<section id="nris-intel-openmp-scaling-on-olivia">
<h3>45.NRIS Intel OpenMP Scaling on Olivia<a class="headerlink" href="#nris-intel-openmp-scaling-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=BdmUxeXKtFc&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=46">In this video</a>, we will perform matrix multiplication using a triple nested loop with OpenMP for parallel computing, utilizing the Intel Fortran compiler loaded through the NRIS module system.
Additionally, we will conduct a scaling analysis of the Fortran program using the triple nested loop with OpenMP on the Olivia Linux cluster.</p>
</section>
<section id="nris-intel-mkl-scaling-on-olivia">
<h3>46.NRIS Intel MKL Scaling on Olivia<a class="headerlink" href="#nris-intel-mkl-scaling-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=uHqbu8PwzjA&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=47">In this video</a>, we will use the MKL routine for matrix multiplication with OpenMP for parallel computing, utilizing the Intel Fortran compiler loaded through the NRIS module system.
Additionally, we will conduct a scaling analysis of the Fortran program using MKL with OpenMP on the Olivia Linux cluster.</p>
</section>
<section id="cray-intel-openmp-scaling-on-olivia">
<h3>47.Cray Intel OpenMP Scaling on Olivia<a class="headerlink" href="#cray-intel-openmp-scaling-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=TKjph4M7R08&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=45">In this video</a>, we will perform matrix multiplication using a triple nested loop with OpenMP for parallel computing, utilizing the Intel Fortran compiler loaded through the Cray module system.
Additionally, we will conduct a scaling analysis of the Fortran program using the triple nested loop with OpenMP on the Olivia Linux cluster.</p>
</section>
<section id="cray-intel-mkl-scaling-on-olivia">
<h3>48.Cray Intel MKL Scaling on Olivia<a class="headerlink" href="#cray-intel-mkl-scaling-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=CuVhnlaXAuw&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=48">In this video</a>, we will use the MKL routine for matrix multiplication with OpenMP for parallel computing, utilizing the Intel Fortran compiler loaded through the Cray module system.
Additionally, we will conduct a scaling analysis of the Fortran program using MKL with OpenMP on the Olivia Linux cluster.</p>
</section>
<section id="summary-of-scaling-on-olivia">
<h3>49.Summary of Scaling on Olivia<a class="headerlink" href="#summary-of-scaling-on-olivia" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://www.youtube.com/watch?v=e7PxXJYkoac&amp;amp;list=PLoR6m-sar9AibHwGSFUZQ9QNxa5dSLl7O&amp;amp;index=49">In this video</a>, we provide a summary of the scaling analysis conducted on Olivia, as detailed in videos 44 through 48. We also compare these results with the scaling analysis performed on Fram and Betzy.
The findings reveal that the new Olivia Linux cluster offers significantly better performance than both Fram and Betzy.</p>
</section>
</section>
<section id="other-topics">
<h2>Other topics<a class="headerlink" href="#other-topics" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="videos.html" class="btn btn-neutral float-left" title="Training Video Archives" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="material.html" class="btn btn-neutral float-right" title="Training materials" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, Sigma2/NRIS. Text shared under CC-BY 4.0 license.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>