

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Olivia Pilot Info &mdash; Sigma2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/nris.css?v=69e7a171" />
      <link rel="stylesheet" type="text/css" href="../_static/universal-navbar.css" />
      <link rel="stylesheet" type="text/css" href="../_static/statuspal.css" />

  
    <link rel="shortcut icon" href="../_static/nris.ico"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://siteimproveanalytics.com/js/siteanalyze_6036825.js"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../_static/statuspal_widget.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">
<!-- Send url to parent when displayed as iframe -->
<script>
    const valid_orign_url = "https://www.sigma2.no"
    window.addEventListener('message', function(event) {
        if (event.data === 'getDocumentationIframeUrl' && event.origin.startsWith(valid_orign_url)) {
            // path only (/path/example.html)
            const path = window.location.pathname
            // query string (including the initial ? symbol)
            const search = window.location.search
            // Returns the hash (including the initial # symbol)
            const hash = window.location.hash
            const newUrl = path + search + hash;
            event.source.postMessage(newUrl, event.origin)
        }
    })

</script>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Sigma2/NRIS documentation
              <img src="../_static/NRIS Logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Policies</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code-of-conduct.html">Code of Conduct</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting help</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/support_line.html">Getting help</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/extended_support.html">Extended support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/faq.html">Frequently asked questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/how_to_write_good_support_requests.html">Writing good support requests</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/qa-sessions.html">Open Question &amp; Answer Sessions for All Users</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/lost_forgotten_password.html">Lost, expiring or changing passwords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/two_factor_authentication.html">One-time-pad (OTP) / Two-factor authentication</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.sigma2.no/project-leader-handbook">Project Leader Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Training</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../training/events.html">Training events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/notes_qa.html">Questions, Answers and Feedbacks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/past_training.html">An overview over training events in the past</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/videos.html">Training Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/short_instructions.html">Short Instructions Video Archives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training/material.html">Training materials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/opslog.html">Status and maintenance of systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/applying_account.html">How do I get an account?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/applying_resources.html">Applying for computing and storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/file_transfer.html">File transfer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/editing_files.html">Editing files</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_development/guides/vs_code/connect_to_server.html">Connecting to a system with Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/ssh.html">SSH</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/ssh.html#common-ssh-errors">Common SSH errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/ood.html">Open OnDemand</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/R.html">First R calculation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Files, storage and backup</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/nird_lmd.html">NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/clusters.html">Storage areas on HPC clusters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/quota.html">Storage quota</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/backup.html">Backup on Betzy, Fram, Saga, and NIRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/sharing_files.html">Data handling and storage policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../files_storage/performance.html">Optimizing storage performance</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">HPC usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/migration2metacenter.html">Migration to an NRIS HPC machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computing/responsible-use.html">Using shared resources responsibly</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jobs/overview.html">Running jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jobs/internet-login-compute-nodes.html">Login nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../jobs/internet-login-compute-nodes.html#compute-nodes">Compute nodes:</a></li>
<li class="toctree-l1"><a class="reference internal" href="../computing/tuning-applications.html">Tuning applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../code_development/guides_llm.html">Running LLM Models in a Cluster Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compute resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/hardware_overview.html">Overview over our machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/betzy.html">Betzy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/fram.html">Fram</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/olivia.html">Olivia</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/saga.html">Saga</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hpc_machines/lumi.html">LUMI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Software</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../software/modulescheme.html">Software module scheme</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/installed_software.html">Installed software</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/userinstallsw.html">Installing software as user</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/appguides.html">Application guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/licenses.html">Licence and access policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/eessi.html">EESSI</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Additional services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../nird_archive/sandbox-user-guide.html">NIRD Research Data Archive Sandbox (NIRD RDA sandbox)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nird_archive/user-guide.html">NIRD Research Data Archive (NIRD RDA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nird_toolkit/overview.html">NIRD Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nird_service_platform/overview_nird_service_platform.html">NIRD Service Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="../services/easydmp-user-documentation.html">EasyDMP User Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_help/course_resources.html">CRaaS - Course Resources as a Service</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code development and tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../code_development/overview.html">Code development and tutorials</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Sigma2/NRIS documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Olivia Pilot Info</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="olivia-pilot-info">
<span id="olivia-pilot-main"></span><h1><a class="toc-backref" href="#id2" role="doc-backlink">Olivia Pilot Info</a><a class="headerlink" href="#olivia-pilot-info" title="Link to this heading"></a></h1>
<!-- **Olivia User Guide ** (Skeleton draft) -->
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p><strong>!! This page is currently under development and work in progress. Configs/information may not be final !!</strong></p>
</div>
<nav class="contents" id="page-overview">
<p class="topic-title"><strong>Page Overview</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#olivia-pilot-info" id="id2">Olivia Pilot Info</a></p>
<ul>
<li><p><a class="reference internal" href="#duration-of-pilot-period" id="id3">Duration of pilot period</a></p></li>
<li><p><a class="reference internal" href="#how-to-connect-to-olivia" id="id4">How to connect to Olivia</a></p></li>
<li><p><a class="reference internal" href="#system-architecture-and-technical-spec" id="id5">System architecture and technical spec</a></p>
<ul>
<li><p><a class="reference internal" href="#cpu-compute-nodes" id="id6">CPU Compute Nodes</a></p></li>
<li><p><a class="reference internal" href="#accelerator-nodes" id="id7">Accelerator Nodes</a></p></li>
<li><p><a class="reference internal" href="#service-nodes" id="id8">Service Nodes</a></p></li>
<li><p><a class="reference internal" href="#i-o-nodes" id="id9">I/O Nodes</a></p></li>
<li><p><a class="reference internal" href="#high-performance-global-storage" id="id10">High-Performance Global Storage</a></p></li>
<li><p><a class="reference internal" href="#high-speed-interconnect" id="id11">High-Speed Interconnect</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#project-accounting-can-be-part-of-general-accounting" id="id12">Project accounting (can be part of general accounting)</a></p></li>
<li><p><a class="reference internal" href="#software" id="id13">Software</a></p>
<ul>
<li><p><a class="reference internal" href="#module-system" id="id14">1. Module System</a></p></li>
<li><p><a class="reference internal" href="#python-r-and-ana-conda" id="id15">2. Python, R, and (Ana-)Conda</a></p></li>
<li><p><a class="reference internal" href="#ai-frameworks" id="id16">3. AI Frameworks</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#running-jobs" id="id17">Running Jobs</a></p>
<ul>
<li><p><a class="reference internal" href="#job-types" id="id18">Job types</a></p></li>
<li><p><a class="reference internal" href="#special-notes" id="id19">Special notes</a></p></li>
<li><p><a class="reference internal" href="#interactive-jobs" id="id20">Interactive jobs</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#internet-access-from-compute-nodes" id="id21">Internet access from compute nodes</a></p></li>
<li><p><a class="reference internal" href="#storage" id="id22">Storage</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="duration-of-pilot-period">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Duration of pilot period</a><a class="headerlink" href="#duration-of-pilot-period" title="Link to this heading"></a></h2>
<p>The pilot period is expected to start on Monday 7. July and will last until 15. September 2025.</p>
</section>
<section id="how-to-connect-to-olivia">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">How to connect to Olivia</a><a class="headerlink" href="#how-to-connect-to-olivia" title="Link to this heading"></a></h2>
<p>Logging into Olivia involves the use of <a class="reference internal" href="../getting_started/ssh.html#ssh"><span class="std std-ref">Secure Shell (SSH)</span></a> protocol,
either in a terminal shell or through a graphical tool using this protocol
under the hood.</p>
<p>In the future, we will introduce the same 2 factor authentication (2FA) system
as used on our other HPC machines. But for now, you cannot connect directly
from your local machine. Instead you have to <strong>first connect to Betzy, Fram or
Saga and then further to Olivia</strong>.</p>
<p>Replace <code class="docutils literal notranslate"><span class="pre">&lt;username&gt;</span></code> with your registered username:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ssh<span class="w"> </span>&lt;username&gt;@betzy.sigma2.no
<span class="gp">$ </span>ssh<span class="w"> </span>&lt;username&gt;@olivia.sigma2.no
</pre></div>
</div>
</section>
<section id="system-architecture-and-technical-spec">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">System architecture and technical spec</a><a class="headerlink" href="#system-architecture-and-technical-spec" title="Link to this heading"></a></h2>
<p>Olivia is based on HPE Cray EX platforms for compute and accelerator nodes, and HPE ClusterStor for global storage, all connected via HPE Slingshot high-speed interconnect.</p>
<section id="cpu-compute-nodes">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">CPU Compute Nodes</a><a class="headerlink" href="#cpu-compute-nodes" title="Link to this heading"></a></h3>
<p>The CPU-based compute resources consist of <strong>252 nodes</strong>. Each node features two <strong>AMD EPYC Turin 128-core CPUs (Zen5c architecture)</strong>, with <strong>768 GiB of DDR5-6000 memory (24x32GB DIMMs)</strong>. Each node also includes <strong>3.84 TB of solid-state local storage (M.2 SSD)</strong>. All Class A nodes are <strong>direct liquid cooled</strong>.</p>
</section>
<section id="accelerator-nodes">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Accelerator Nodes</a><a class="headerlink" href="#accelerator-nodes" title="Link to this heading"></a></h3>
<p>These nodes are optimized for accelerated HPC and AI/ML workloads, totaling <strong>76 nodes with 304 NVIDIA GH200 superchips</strong>. Each GH200 device has <strong>96 GB HBM3 memory (GPU)</strong> and <strong>120 GB LPDDR5x memory (CPU)</strong>, connected via a <strong>900 GB/s bidirectional NVlink C2C connection</strong>. These nodes are also <strong>direct liquid cooled</strong>. Their dedicated storage is provided by a <strong>HPE Cray C500 flash-based parallel filesystem with a total capacity of 510 TB</strong>.</p>
</section>
<section id="service-nodes">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Service Nodes</a><a class="headerlink" href="#service-nodes" title="Link to this heading"></a></h3>
<p>Olivia has <strong>8 service nodes</strong>, with 4 nodes having both CPU and GPU (<strong>NVIDIA L40 GPUs</strong> for remote visualization and compilation). Each node has <strong>2 AMD EPYC 9534 64-core processors</strong> and <strong>1536 GiB DDR5-4800 memory</strong>. Storage per node includes <strong>2x 960 GB NVMe SSDs for the system drive</strong> (configured as RAID mirror for redundancy) and <strong>4x 6.4 TB NVMe SSDs for data (19.2 TB total)</strong>. These nodes are <strong>air-cooled</strong>.</p>
</section>
<section id="i-o-nodes">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">I/O Nodes</a><a class="headerlink" href="#i-o-nodes" title="Link to this heading"></a></h3>
<p>Olivia has <strong>5 dedicated nodes for I/O operations</strong>, based on HPE ProLiant DL385 Gen 11. Each node has <strong>2 AMD EPYC Genoa 9534 64-core CPUs</strong> and <strong>512 GiB memory</strong>. They are designed to connect to the NIRD storage system and the Global Storage simultaneously. These nodes are <strong>air-cooled</strong>.</p>
</section>
<section id="high-performance-global-storage">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">High-Performance Global Storage</a><a class="headerlink" href="#high-performance-global-storage" title="Link to this heading"></a></h3>
<p>This system, based on ClusterStor E1000, offers <strong>4.266 PB of total usable capacity</strong>, including <strong>1.107 PB of solid-state storage (NVMe)</strong> and <strong>3.159 PB of HDD-based storage</strong>. It is POSIX compatible and can be divided into several file systems (e.g., scratch, home, projects, software). It supports RDMA and is designed for fault tolerance against single component failures. The system is <strong>air-cooled</strong>.</p>
</section>
<section id="high-speed-interconnect">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">High-Speed Interconnect</a><a class="headerlink" href="#high-speed-interconnect" title="Link to this heading"></a></h3>
<p>The interconnect uses <strong>HPE Slingshot</strong>, a Dragonfly topology that supports <strong>200 Gbit/s simplex bandwidth</strong> between nodes. It natively supports IP over Ethernet, and RDMA or RoCE access to the storage solution.</p>
</section>
</section>
<section id="project-accounting-can-be-part-of-general-accounting">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Project accounting (can be part of general accounting)</a><a class="headerlink" href="#project-accounting-can-be-part-of-general-accounting" title="Link to this heading"></a></h2>
<p><strong>Invoicing</strong>: During the piloting phase there will no invoicing for usage of the system.</p>
<p>Project compute quota usage is accounted in ‘billing units’ BU. The calculation of elapsed BU for a job is done in the same manner as elsewhere on our systems.</p>
<p>The calculation of elapsed GPU-hours is currently only dependent on the number of GPUs allocated for a job.</p>
</section>
<section id="software">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">Software</a><a class="headerlink" href="#software" title="Link to this heading"></a></h2>
<p>Olivia introduces several new aspects that set it apart from our current HPC systems.</p>
<p>First and foremost, the large number of GPU nodes and the software that will run on them represent uncharted territory for us, as we don’t yet have extensive experience in this area. Additionally, Olivia is an HPE Cray machine that leverages the <a class="reference external" href="https://cpe.ext.hpe.com/docs/latest/getting_started/CPE-General-User-Guide-HPCM.html">Cray Programming Environment (CPE)</a>, which comes with its own suite of tools for development, compilation, and debugging. As a result, we cannot simply replicate the software stack from our other systems. Instead, we need to adapt, modify, and expand the ways in which software is installed and utilized.</p>
<p>The pilot phase will serve as a testing ground for the approaches outlined below, helping us identify the best solutions for providing software on Olivia. Please note that <strong>things will evolve during the pilot phase</strong>, and the final configuration may differ significantly from what is described here. Your feedback is invaluable—if something doesn’t work or could be improved, let us know so we can make adjustments.</p>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p><strong>Python and PyTorch user</strong></p>
<p>Please read the sections 2 and 3 below.
Do not use Anaconda or pip directly to install large environments with many packages.</p>
</div>
<section id="module-system">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">1. Module System</a><a class="headerlink" href="#module-system" title="Link to this heading"></a></h3>
<p>The module system provides a convenient way to access software packages and libraries that are installed and maintained specifically for the Olivia HPC cluster. All software is compiled and optimized for Olivia’s nodes to ensure the best performance.</p>
<section id="cpu-and-gpu-architectures">
<h4>CPU and GPU Architectures<a class="headerlink" href="#cpu-and-gpu-architectures" title="Link to this heading"></a></h4>
<p>Olivia features three distinct CPU architectures:</p>
<ul class="simple">
<li><p><strong>Login Nodes</strong>: Use x86-64 processors with the Zen4 microarchitecture.</p></li>
<li><p><strong>CPU Compute Nodes</strong>: Use x86-64 processors with the Zen5 microarchitecture.</p></li>
<li><p><strong>GPU Nodes</strong>: Use ARM-based Neoverse V2 processors.</p></li>
</ul>
<p>To accommodate these architectures, we have prepared three separate software stacks. You can initialize the module system and access these stacks by running the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>purge
$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>/opt/cray/pe/lmod/lmod/init/profile
$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">MODULEPATH</span><span class="o">=</span>/cluster/software/modules/Core/
</pre></div>
</div>
<p>After initialization, you can list the available stacks using:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>available
</pre></div>
</div>
<p>You should see output similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">---------------------</span> <span class="o">/</span><span class="n">cluster</span><span class="o">/</span><span class="n">software</span><span class="o">/</span><span class="n">modules</span><span class="o">/</span><span class="n">Core</span> <span class="o">-----------</span>
   <span class="n">BuildEnv</span><span class="o">/</span><span class="n">NeoverseV2</span>    <span class="n">BuildEnv</span><span class="o">/</span><span class="n">Zen5</span> <span class="p">(</span><span class="n">D</span><span class="p">)</span>    <span class="n">NRIS</span><span class="o">/</span><span class="n">GPU</span>   <span class="p">(</span><span class="n">S</span><span class="p">)</span>
   <span class="n">BuildEnv</span><span class="o">/</span><span class="n">Zen4</span>          <span class="n">NRIS</span><span class="o">/</span><span class="n">CPU</span>      <span class="p">(</span><span class="n">S</span><span class="p">)</span>    <span class="n">NRIS</span><span class="o">/</span><span class="n">Login</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>

  <span class="n">Where</span><span class="p">:</span>
   <span class="n">S</span><span class="p">:</span>  <span class="n">Module</span> <span class="ow">is</span> <span class="n">Sticky</span><span class="p">,</span> <span class="n">requires</span> <span class="o">--</span><span class="n">force</span> <span class="n">to</span> <span class="n">unload</span> <span class="ow">or</span> <span class="n">purge</span>
   <span class="n">D</span><span class="p">:</span>  <span class="n">Default</span> <span class="n">Module</span>
</pre></div>
</div>
<p>If the list of available modules is too long, you can use the following commands for a more focused view:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">--default</span> <span class="pre">avail</span></code> or <code class="docutils literal notranslate"><span class="pre">ml</span> <span class="pre">-d</span> <span class="pre">av</span></code>: Lists only the default modules.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">overview</span></code> or <code class="docutils literal notranslate"><span class="pre">ml</span> <span class="pre">ov</span></code>: Displays the number of modules for each name.</p></li>
</ul>
<p>To search for specific modules or extensions, use:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code>: Finds all possible modules and extensions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">keyword</span> <span class="pre">key1</span> <span class="pre">key2</span> <span class="pre">...</span></code>: Searches for modules matching specific keywords.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="nris-software-stacks">
<h4>NRIS Software Stacks<a class="headerlink" href="#nris-software-stacks" title="Link to this heading"></a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">NRIS</span></code> modules provide preinstalled software tailored for different node types:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">NRIS/CPU</span></code></strong>: Contains software packages and libraries optimized for the <strong>CPU compute nodes</strong>.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">NRIS/GPU</span></code></strong>: Currently includes libraries, compilers, and tools for building software for the Grace-Hopper 200 GPUs. In the future, this stack will also include AI frameworks such as PyTorch and TensorFlow (see <em>AI Frameworks</em> below).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">NRIS/Login</span></code></strong>: Includes tools for pre- and post-processing data.
<strong>Note</strong>: Do not use this stack for running workflows on the compute nodes.</p></li>
</ul>
<p>To load a stack and view its available software, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>NRIS/CPU
$<span class="w"> </span>module<span class="w"> </span>avail
</pre></div>
</div>
<hr class="docutils" />
<section id="searching-for-modules-across-stacks">
<h5>Searching for Modules Across Stacks<a class="headerlink" href="#searching-for-modules-across-stacks" title="Link to this heading"></a></h5>
<p>You can search for a specific module across all stacks using <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code>. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>spider<span class="w"> </span>SCOTCH
</pre></div>
</div>
<p>This will display information about the <code class="docutils literal notranslate"><span class="pre">SCOTCH</span></code> module, including available versions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-----------------------------------------------------------</span>
  <span class="n">SCOTCH</span><span class="p">:</span>
<span class="o">-----------------------------------------------------------</span>
    <span class="n">Description</span><span class="p">:</span>
      <span class="n">Software</span> <span class="n">package</span> <span class="ow">and</span> <span class="n">libraries</span> <span class="k">for</span> <span class="n">sequential</span> <span class="ow">and</span>
      <span class="n">parallel</span> <span class="n">graph</span> <span class="n">partitioning</span><span class="p">,</span> <span class="n">static</span> <span class="n">mapping</span><span class="p">,</span> <span class="ow">and</span>
      <span class="n">sparse</span> <span class="n">matrix</span> <span class="n">block</span> <span class="n">ordering</span><span class="p">,</span> <span class="ow">and</span> <span class="n">sequential</span> <span class="n">mesh</span> <span class="ow">and</span>
      <span class="n">hypergraph</span> <span class="n">partitioning</span><span class="o">.</span>

     <span class="n">Versions</span><span class="p">:</span>
        <span class="n">SCOTCH</span><span class="o">/</span><span class="mf">7.0.3</span><span class="o">-</span><span class="n">gompi</span><span class="o">-</span><span class="mi">2023</span><span class="n">a</span>
        <span class="n">SCOTCH</span><span class="o">/</span><span class="mf">7.0.4</span><span class="o">-</span><span class="n">gompi</span><span class="o">-</span><span class="mi">2023</span><span class="n">b</span>
        <span class="n">SCOTCH</span><span class="o">/</span><span class="mf">7.0.6</span><span class="o">-</span><span class="n">gompi</span><span class="o">-</span><span class="mi">2024</span><span class="n">a</span>
</pre></div>
</div>
<p>To get detailed information about a specific version, including how to load it, use the module’s full name:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>spider<span class="w"> </span>SCOTCH/7.0.6-gompi-2024a
</pre></div>
</div>
<p>Similarly, for other modules like <code class="docutils literal notranslate"><span class="pre">NVHPC</span></code>, you can use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>spider<span class="w"> </span>NVHPC/25.3-CUDA-12.8.0
</pre></div>
</div>
<p>This will provide details such as dependencies and additional help:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-----------------------------------------------------------</span>
  <span class="n">NVHPC</span><span class="p">:</span> <span class="n">NVHPC</span><span class="o">/</span><span class="mf">25.3</span><span class="o">-</span><span class="n">CUDA</span><span class="o">-</span><span class="mf">12.8.0</span>
<span class="o">-----------------------------------------------------------</span>
    <span class="n">Description</span><span class="p">:</span>
      <span class="n">C</span><span class="p">,</span> <span class="n">C</span><span class="o">++</span> <span class="ow">and</span> <span class="n">Fortran</span> <span class="n">compilers</span> <span class="n">included</span> <span class="k">with</span> <span class="n">the</span> <span class="n">NVIDIA</span>
      <span class="n">HPC</span> <span class="n">SDK</span> <span class="p">(</span><span class="n">previously</span><span class="p">:</span> <span class="n">PGI</span><span class="p">)</span>

    <span class="n">You</span> <span class="n">will</span> <span class="n">need</span> <span class="n">to</span> <span class="n">load</span> <span class="nb">all</span> <span class="n">module</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">on</span> <span class="nb">any</span> <span class="n">one</span> <span class="n">of</span> <span class="n">the</span>
    <span class="n">lines</span> <span class="n">below</span> <span class="n">before</span> <span class="n">the</span> <span class="s2">&quot;NVHPC/25.3-CUDA-12.8.0&quot;</span> <span class="n">module</span> <span class="ow">is</span>
    <span class="n">available</span> <span class="n">to</span> <span class="n">load</span><span class="o">.</span>

      <span class="n">BuildEnv</span><span class="o">/</span><span class="n">NeoverseV2</span>
      <span class="n">NRIS</span><span class="o">/</span><span class="n">GPU</span>

    <span class="n">Help</span><span class="p">:</span>
      <span class="n">Description</span>
      <span class="o">===========</span>
      <span class="n">C</span><span class="p">,</span> <span class="n">C</span><span class="o">++</span> <span class="ow">and</span> <span class="n">Fortran</span> <span class="n">compilers</span> <span class="n">included</span> <span class="k">with</span> <span class="n">the</span> <span class="n">NVIDIA</span>
      <span class="n">HPC</span> <span class="n">SDK</span> <span class="p">(</span><span class="n">previously</span><span class="p">:</span> <span class="n">PGI</span><span class="p">)</span>

      <span class="n">More</span> <span class="n">information</span>
      <span class="o">================</span>
       <span class="o">-</span> <span class="n">Homepage</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">developer</span><span class="o">.</span><span class="n">nvidia</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">hpc</span><span class="o">-</span><span class="n">sdk</span><span class="o">/</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="important-notes">
<h5>Important Notes<a class="headerlink" href="#important-notes" title="Link to this heading"></a></h5>
<ul class="simple">
<li><p><strong>Avoid Using <code class="docutils literal notranslate"><span class="pre">BuildEnv</span></code> Modules</strong>: These modules are intended for internal use and will be hidden in the future. Please use the <code class="docutils literal notranslate"><span class="pre">NRIS</span></code> modules instead.</p></li>
<li><p><strong>Software Requests</strong>: We will install additional software packages in the future. If you cannot find the software you need, please contact us. Refer to our <a class="reference internal" href="../getting_help/support_line.html#support-line"><span class="std std-ref">Getting Help page</span></a> for details, and be sure to mention that your request is regarding Olivia.</p></li>
<li><p>For additional help, consult the module system’s built-in commands (<code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">help</span></code>, <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">spider</span></code>, etc.) or contact the system administrators.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="using-the-eessi-stack">
<h4>Using the EESSI Stack<a class="headerlink" href="#using-the-eessi-stack" title="Link to this heading"></a></h4>
<p>The EESSI (European Environment for Scientific Software Infrastructure) software stack - optimized for each supported CPU architecture - already available on Betzy, Fram, and Saga is now also available on Olivia.</p>
<p>To load the EESSI stack, simply use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>/cluster/installations/eessi/default/eessi_environment_variables_Olivia
$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>/cvmfs/software.eessi.io/versions/2023.06/init/bash
</pre></div>
</div>
<p>The second script automatically detects the CPU microarchitecture of the current machine and selects the most appropriate pre-built software provided by EESSI. This ensures optimal performance and works seamlessly across systems with varying CPU architectures, such as those available on Olivia.</p>
<p>Once the environment is configured to access software provided by EESSI, you can interact with it using the <code class="docutils literal notranslate"><span class="pre">module</span></code> command, just like with the NRIS stacks. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>avail
$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>TensorFlow/2.13.0-foss-2023a
</pre></div>
</div>
<p>The first command will list all software modules available within the EESSI stack (on the current CPU/GPU partition).
Then we load the <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> module.</p>
<p>While EESSI provides a wide range of preinstalled software, you can <strong>build</strong> on top of EESSI either using EasyBuild through loading the EESSI-extend module:
<code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">EESSI-extend</span></code> or without EasyBuild through loading one of the available <code class="docutils literal notranslate"><span class="pre">buildenv/*</span></code> modules, for example, <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">buildenv/default-foss-2023a</span></code>
For more information see the official <a class="reference external" href="https://www.EESSI.do/docs/using_EESSI/building_on_EESSI/">EESSI documentation</a>.</p>
<section id="gpu-enabled-software-on-eessi">
<h5>GPU-enabled Software on EESSI<a class="headerlink" href="#gpu-enabled-software-on-eessi" title="Link to this heading"></a></h5>
<p>The official EESSI stack contains already some modules of popular software like GROMACS, but many are also still missing.</p>
<p>To get you started more quickly, we have added some GPU-enabled software on Olivia which are not yet provided by <a class="reference external" href="https://www.eessi.io/docs">EESSI</a> or supported by <a class="reference external" href="https://easybuild.io/">EasyBuild</a>.</p>
<p>To access it, run the following command after initializing the EESSI environment on a <strong>GPU node</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">MODULEPATH</span><span class="o">=</span>/cluster/installations/eessi/default/eessi_local/aarch64/modules/all:<span class="nv">$MODULEPATH</span>
</pre></div>
</div>
<p>Note: If your job requires the use of <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>, please ensure you use the <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">--mpi=pmix</span></code> option for proper integration.
Sample job script using <code class="docutils literal notranslate"><span class="pre">OSU-Micro-Benchmarks/7.2-gompi-2023b</span></code> from the EESSI stack:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash -e</span>
<span class="c1">#SBATCH --job-name=Sample</span>
<span class="c1">#SBATCH --account=nnXXXX</span>
<span class="c1">#SBATCH --time=00:05:00</span>
<span class="c1">#SBATCH --partition=normal</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --ntasks-per-node=1</span>

<span class="nb">source</span><span class="w"> </span>/cluster/installations/eessi/default/eessi_environment_variables_Olivia
<span class="nb">source</span><span class="w"> </span>/cvmfs/software.eessi.io/versions/2023.06/init/bash
module<span class="w"> </span>load<span class="w"> </span>OSU-Micro-Benchmarks/7.2-gompi-2023b

srun<span class="w"> </span>--mpi<span class="o">=</span>pmix<span class="w"> </span>osu_bw
</pre></div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="python-r-and-ana-conda">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">2. Python, R, and (Ana-)Conda</a><a class="headerlink" href="#python-r-and-ana-conda" title="Link to this heading"></a></h3>
<p>Python and R are widely used in scientific computing, but they were originally designed for personal computers rather than high-performance computing (HPC) environments. These languages often involve installations with a large number of small files—Python environments, for example, can easily consist of tens or even hundreds of thousands of files. This can strain the file system, leading to poor performance and a sluggish user experience.</p>
<p>To address this, Olivia uses the <a class="reference external" href="https://github.com/CSCfi/hpc-container-wrapper/"><em>HPC-container-wrapper</em></a>, a tool designed to encapsulate installations within containers optimized for HPC systems. This tool wraps Python or Conda environments inside containers, significantly reducing the number of files visible to the file system. It also generates executables, allowing you to run commands like <code class="docutils literal notranslate"><span class="pre">python</span></code> seamlessly, without needing to interact directly with the container. This approach minimizes file system load while maintaining ease of use.</p>
<hr class="docutils" />
<section id="key-features-of-hpc-container-wrapper">
<h4>Key Features of HPC-container-wrapper<a class="headerlink" href="#key-features-of-hpc-container-wrapper" title="Link to this heading"></a></h4>
<p>The HPC-container-wrapper supports wrapping:</p>
<ul class="simple">
<li><p><strong>Conda installations</strong>: Based on a <a class="reference external" href="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#sharing-an-environment">Conda environment file</a>.</p></li>
<li><p><strong>Pip installations</strong>: Based on a <a class="reference external" href="https://pip.pypa.io/en/latest/reference/requirements-file-format/">pip requirements.txt file</a>.</p></li>
<li><p><strong>Existing installations on the filesystem</strong>: To reduce I/O load and improve startup times.</p></li>
<li><p><strong>Existing Singularity/Apptainer containers</strong>: To hide the need for using the container runtime from the user.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="creating-a-new-python-or-conda-environment">
<h4>Creating a New Python or Conda Environment<a class="headerlink" href="#creating-a-new-python-or-conda-environment" title="Link to this heading"></a></h4>
<p>To create a new Python or Conda environment on Olivia, follow these steps:</p>
<ol class="arabic">
<li><p><strong>Load the necessary modules</strong>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>module<span class="w"> </span>purge
$<span class="w"> </span><span class="nb">source</span><span class="w"> </span>/opt/cray/pe/lmod/lmod/init/profile
$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">MODULEPATH</span><span class="o">=</span>/cluster/software/modules/Core/
$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>NRIS/CPU
$<span class="w"> </span>module<span class="w"> </span>load<span class="w"> </span>hpc-container-wrapper
</pre></div>
</div>
</li>
<li><p><strong>Prepare your environment file</strong>:</p>
<ul>
<li><p>For <strong>pip</strong>, create a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code> file listing the packages you need. For example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>numpy==1.23.5
scipy==1.10.1
matplotlib
</pre></div>
</div>
</li>
<li><p>For <strong>Conda</strong>, create an <code class="docutils literal notranslate"><span class="pre">env.yml</span></code> file. For example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">channels</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">conda-forge</span>
<span class="nt">dependencies</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python=3.10</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">numpy</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pandas</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">matplotlib</span>
</pre></div>
</div>
</li>
</ul>
<p>Alternatively, you can export an environment from an existing setup (e.g., on your laptop or another cluster):</p>
<ul>
<li><p>Export a Conda environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda<span class="w"> </span>env<span class="w"> </span><span class="nb">export</span><span class="w"> </span>-n<span class="w"> </span>&lt;env_name&gt;<span class="w"> </span>&gt;<span class="w"> </span>env.yml
</pre></div>
</div>
<p><em>Note</em>: On Windows or macOS, add the <code class="docutils literal notranslate"><span class="pre">--from-history</span></code> flag to avoid including platform-specific dependencies.</p>
</li>
<li><p>Export a pip environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>pip<span class="w"> </span>freeze<span class="w"> </span>&gt;<span class="w"> </span>requirements.txt
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Build the environment</strong>:</p>
<ul>
<li><p>For <strong>pip</strong>, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>pip-containerize<span class="w"> </span>new<span class="w"> </span>--prefix<span class="w"> </span>&lt;install_dir&gt;<span class="w"> </span>--slim<span class="w"> </span>requirements.txt
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">--slim</span></code> argument uses a pre-built minimal Python container with a newer Python version as a base. Without <code class="docutils literal notranslate"><span class="pre">--slim</span></code>, the host system is fully available, but this may include unnecessary system installations.</p>
</li>
<li><p>For <strong>Conda</strong>, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda-containerize<span class="w"> </span>new<span class="w"> </span>--prefix<span class="w"> </span>&lt;install_dir&gt;<span class="w"> </span>env.yml
</pre></div>
</div>
<p>If you also need to install additional pip packages, you can combine both files:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda-containerize<span class="w"> </span>new<span class="w"> </span>-r<span class="w"> </span>requirements.txt<span class="w"> </span>--prefix<span class="w"> </span>&lt;install_dir&gt;<span class="w"> </span>env.yml
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><strong>Add the environment to your PATH</strong>:</p>
<p>After the installation is complete, add the <code class="docutils literal notranslate"><span class="pre">bin</span></code> directory of your environment to your <code class="docutils literal notranslate"><span class="pre">PATH</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;&lt;install_dir&gt;/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>You can now call <code class="docutils literal notranslate"><span class="pre">python</span></code> or any other executables installed in the environment as if the environment were activated.</p>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="modifying-an-existing-environment">
<h4>Modifying an Existing Environment<a class="headerlink" href="#modifying-an-existing-environment" title="Link to this heading"></a></h4>
<p>Since the environment is wrapped inside a container, direct modifications are not possible. However, you can update the environment using the <code class="docutils literal notranslate"><span class="pre">update</span></code> keyword along with a post-installation script. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda-containerize<span class="w"> </span>update<span class="w"> </span>&lt;install_dir&gt;<span class="w"> </span>--post-install<span class="w"> </span>&lt;script.sh&gt;
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&lt;script.sh&gt;</span></code> file might contain commands like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>seaborn
$<span class="w"> </span>conda<span class="w"> </span>remove<span class="w"> </span>-y<span class="w"> </span>pyyaml
$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>requests
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="tips-and-troubleshooting">
<h4>Tips and Troubleshooting<a class="headerlink" href="#tips-and-troubleshooting" title="Link to this heading"></a></h4>
<ul>
<li><p><strong>Exporting environments</strong>: If you encounter issues with version conflicts, you can remove version specifications from your environment file using a simple <code class="docutils literal notranslate"><span class="pre">sed</span></code> command. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>sed<span class="w"> </span><span class="s1">&#39;/==/s/==.*//&#39;</span><span class="w"> </span>requirements.txt<span class="w"> </span>&gt;<span class="w"> </span>requirements_versionless.txt<span class="w">  </span><span class="c1"># Works for pip files</span>
$<span class="w"> </span>sed<span class="w"> </span><span class="s1">&#39;/=/s/=.*//&#39;</span><span class="w"> </span>env.yml<span class="w"> </span>&gt;<span class="w"> </span>env_versionless.yml<span class="w">  </span><span class="c1"># Works for conda files</span>
</pre></div>
</div>
</li>
<li><p><strong>Using Mamba</strong>: For faster Conda installations, you can enable <a class="reference external" href="https://github.com/mamba-org/mamba">Mamba</a> by adding the <code class="docutils literal notranslate"><span class="pre">--mamba</span></code> flag:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda-containerize<span class="w"> </span>new<span class="w"> </span>--mamba<span class="w"> </span>--prefix<span class="w"> </span>&lt;install_dir&gt;<span class="w"> </span>env.yml
</pre></div>
</div>
</li>
<li><p><strong>Limitations</strong>: Be aware of the <a class="reference external" href="https://github.com/CSCfi/hpc-container-wrapper#limitations">limitations of HPC-container-wrapper</a>, such as its experimental status and potential issues with advanced features.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="example-workflow-end-to-end-conda-installation">
<h4>Example Workflow: End-to-End Conda Installation<a class="headerlink" href="#example-workflow-end-to-end-conda-installation" title="Link to this heading"></a></h4>
<ol class="arabic">
<li><p>Create an <code class="docutils literal notranslate"><span class="pre">env.yml</span></code> file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">channels</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">conda-forge</span>
<span class="nt">dependencies</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python=3.9</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">numpy</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">scipy</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">matplotlib</span>
</pre></div>
</div>
</li>
<li><p>Build the environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda-containerize<span class="w"> </span>new<span class="w"> </span>--prefix<span class="w"> </span>MyEnv<span class="w"> </span>env.yml
</pre></div>
</div>
</li>
<li><p>Add the environment to your <code class="docutils literal notranslate"><span class="pre">PATH</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="s2">&quot;</span><span class="nv">$PWD</span><span class="s2">/MyEnv/bin:</span><span class="nv">$PATH</span><span class="s2">&quot;</span>
</pre></div>
</div>
</li>
<li><p>Use the environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python<span class="w"> </span>--version
Python<span class="w"> </span><span class="m">3</span>.9.0
</pre></div>
</div>
</li>
</ol>
<hr class="docutils" />
<p>For more examples and advanced usage, see the <a class="reference external" href="https://github.com/CSCfi/hpc-container-wrapper">HPC-container-wrapper GitHub repository</a> and check the <a class="reference external" href="https://docs.lumi-supercomputer.eu/software/installing/container-wrapper/#wrapping-a-plain-pip-installation">documentation page of CSC about HPC-container-wrapper</a></p>
</section>
</section>
<hr class="docutils" />
<section id="ai-frameworks">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">3. AI Frameworks</a><a class="headerlink" href="#ai-frameworks" title="Link to this heading"></a></h3>
<p>For AI workflows based on popular frameworks like PyTorch, JAX, or TensorFlow, we aim to deliver optimal performance by utilizing containers provided by <a class="reference external" href="https://catalog.ngc.nvidia.com/containers">NVIDIA</a>. These containers are specifically optimized for GPU workloads, ensuring excellent performance while remaining relatively straightforward to use.</p>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p>We have done testing and have recommendations for using PyTorch using Python wheels.
If you want to use PyTorch wheels during the test phase, you can refer to this <a class="reference internal" href="../code_development/guides/olivia_pytorch.html#pytorch-olivia"><span class="std std-ref">this documentation</span></a>.</p>
<p>But please be aware that direct (not containerized) installations of pip and conda environments, put a lot of stress on the Lustre file system.
Therefore, these will not be allowed after the pilot phase. Instead you have to either use the containers and modules we provide or wrap your installation yourself imanually or using the hpc-container-wrapper as explained above.</p>
</div>
<hr class="docutils" />
<section id="downloading-containers">
<h4>Downloading Containers<a class="headerlink" href="#downloading-containers" title="Link to this heading"></a></h4>
<p>You can use pre-built containers or download and convert your own. Here are the options:</p>
<ol class="arabic">
<li><p><strong>Pre-Built Containers</strong>:<br />
Some NVIDIA containers are already available in the Apptainer image format (<code class="docutils literal notranslate"><span class="pre">.sif</span></code>) under:<br />
<code class="docutils literal notranslate"><span class="pre">/cluster/work/support/container</span></code>.</p></li>
<li><p><strong>Downloading Your Own Containers</strong>:<br />
You can download containers from sources like the <a class="reference external" href="https://catalog.ngc.nvidia.com/containers">NVIDIA NCC catalogue</a> or <a class="reference external" href="https://hub.docker.com/">Docker Hub</a>. Use the following command to download and convert a container into the Apptainer format:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span>build<span class="w"> </span>&lt;image_name&gt;.sif<span class="w"> </span>docker://&lt;docker_image_url&gt;
</pre></div>
</div>
<p>For example, to download and convert the latest NVIDIA PyTorch container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span>build<span class="w"> </span>pytorch_nvidia_25.06.sif<span class="w"> </span>docker://nvcr.io/nvidia/pytorch:25.06-py3
</pre></div>
</div>
<p>For more options, check the Apptainer build help:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span>build<span class="w"> </span>--help
</pre></div>
</div>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id1">
<h4>Important Notes<a class="headerlink" href="#id1" title="Link to this heading"></a></h4>
<ul>
<li><p><strong>Building Your Own Containers:</strong>
Currently it is only possible to build containers from definition files that don’t require root privileges inside the container. We are working on enabling some version of fakeroot to enable most builds in the future.</p></li>
<li><p><strong>Running on GPU Nodes</strong>:<br />
If you plan to run these containers on GPU nodes, ensure that you download the container from a GPU node. This can be done either via a job script or an interactive job. This ensures that the container is built for the correct architecture (e.g., ARM64).<br />
For details on accessing internet resources from compute nodes, see <a class="reference internal" href="#olivia-internet-proxies"><span class="std std-ref">this section</span></a>.</p></li>
<li><p><strong>Managing Cache</strong>:<br />
By default, Apptainer stores its cache in your home directory, which may lead to <code class="docutils literal notranslate"><span class="pre">disk</span> <span class="pre">quota</span> <span class="pre">exceeded</span></code> errors. To avoid this, set the cache directory to your project work area:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">APPTAINER_CACHEDIR</span><span class="o">=</span>/cluster/work/projects/&lt;project_number&gt;/singularity
</pre></div>
</div>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="running-containers">
<h4>Running Containers<a class="headerlink" href="#running-containers" title="Link to this heading"></a></h4>
<p>Apptainer provides several ways to run containers, depending on your needs:</p>
<ol class="arabic">
<li><p><strong>Run the Default Command</strong>:<br />
Use <code class="docutils literal notranslate"><span class="pre">apptainer</span> <span class="pre">run</span></code> to execute the default command specified in the container setup. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span>run<span class="w"> </span>&lt;image_path&gt;.sif<span class="w"> </span>&lt;extra_parameters&gt;
</pre></div>
</div>
<p><em>Example</em>: Running a containerized application with its default configuration.</p>
</li>
<li><p><strong>Execute Arbitrary Commands</strong>:<br />
Use <code class="docutils literal notranslate"><span class="pre">apptainer</span> <span class="pre">exec</span></code> to run specific commands available inside the container. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>&lt;image_path&gt;.sif<span class="w"> </span>&lt;command&gt;<span class="w"> </span>&lt;extra_parameters&gt;
</pre></div>
</div>
<p><em>Example</em>: Running a Python script inside the container.</p>
</li>
<li><p><strong>Interactive Shell</strong>:<br />
Use <code class="docutils literal notranslate"><span class="pre">apptainer</span> <span class="pre">shell</span></code> to start an interactive shell session inside the container. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span>shell<span class="w"> </span>&lt;image_path&gt;.sif<span class="w"> </span>&lt;extra_parameters&gt;
</pre></div>
</div>
<p><em>Example</em>: Exploring the container environment or debugging.</p>
</li>
<li><p><strong>Job Script Setup for multi GPU (single node/multi node)</strong>:</p>
<p>While experimenting, we encountered cases where the <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> was not recognized unless its full path was explicitly specified. We can set the path to the <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> as <code class="docutils literal notranslate"><span class="pre">TORCHRUN_PATH=&quot;/usr/local/bin/torchrun</span></code>, then bind it with the apptainer exec command as <code class="docutils literal notranslate"><span class="pre">apptainer</span> <span class="pre">exec</span> <span class="pre">--nv</span> <span class="pre">--bind</span>&#160; <span class="pre">$TORCHRUN_PATH</span> <span class="pre">./your_script.py</span></code>.</p>
<p>Moreover , if we need GPUs across multiple nodes, we have to take into consideration that some of the framework like Libfabric might not be installed on the container. So, we need to explicitly bind it so that our container could be use to run on multiple nodes.</p>
<p>In our host system, we identified that the libfabric is available at this location <code class="docutils literal notranslate"><span class="pre">/opt/cray/libfabric/1.22.0/lib64</span></code> . The code written below on the job script will add <code class="docutils literal notranslate"><span class="pre">--bind</span> <span class="pre">/opt/cray/libfabric/1.22.0/lib64:/usr/lib64</span></code> to the apptainer command which ensures that the libfabric libraries from the host system are available inside the container at `/usr/lib64``</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bind libfabric (adjust the path based on your host system)</span>
<span class="nv">LIBFABRIC_PATH</span><span class="o">=</span><span class="s2">&quot;/opt/cray/libfabric/1.22.0/lib64&quot;</span>

<span class="c1"># Explicitly specify the full path to torchrun</span>
<span class="nv">TORCHRUN_PATH</span><span class="o">=</span><span class="s2">&quot;/usr/local/bin/torchrun&quot;</span>

<span class="c1"># Run the training script with torchrun inside the container</span>
srun<span class="w"> </span>apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>--bind<span class="w">  </span><span class="nv">$LIBFABRIC_PATH</span>:/usr/lib64<span class="w">  </span><span class="nv">$TORCHRUN_PATH</span><span class="w"> </span>--nnodes<span class="o">=</span><span class="nv">$SLURM_JOB_NUM_NODES</span><span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="nv">$SLURM_GPUS_ON_NODE</span><span class="w"> </span>--rdzv_id<span class="o">=</span><span class="nv">$RANDOM</span><span class="w"> </span>.....
</pre></div>
</div>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="enabling-gpu-support">
<h4>Enabling GPU Support<a class="headerlink" href="#enabling-gpu-support" title="Link to this heading"></a></h4>
<p>To enable GPU support inside the container, add the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag to your Apptainer command. This ensures that the container has access to the GPU resources. For example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>apptainer<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>/cluster/work/support/container/pytorch_nvidia_25.06_arm64.sif<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch; print(torch.cuda.is_available()); print(torch.cuda.device_count())&#39;</span>
</pre></div>
</div>
<p>This command checks if GPUs are available and prints the number of GPUs detected inside the container.</p>
</section>
<hr class="docutils" />
<section id="next-steps">
<h4>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h4>
<p>Currently, you need to run these containers directly using Apptainer. However, we are working on simplifying the experience by integrating these tools into the module system. This will allow you to:</p>
<ul class="simple">
<li><p>Load a module that provides executables like <code class="docutils literal notranslate"><span class="pre">python</span></code> and <code class="docutils literal notranslate"><span class="pre">torch</span></code>, eliminating the need to interact with the containers directly.</p></li>
<li><p>Seamlessly use AI frameworks without worrying about container management.</p></li>
</ul>
<p>Additionally, we will soon provide more information on how to best utilize the interconnect and set up job scripts for running AI workflows efficiently.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="running-jobs">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Running Jobs</a><a class="headerlink" href="#running-jobs" title="Link to this heading"></a></h2>
<section id="job-types">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">Job types</a><a class="headerlink" href="#job-types" title="Link to this heading"></a></h3>
<p>For the pilot period of Olivia, the following job types are defined.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Name</p></th>
<th class="head"><p>Description</p></th>
<th class="head text-center"><p>Job limits</p></th>
<th class="head text-center"><p>Max walltime</p></th>
<th class="head text-center"><p>Priority</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>normal</p></td>
<td><p>default job type</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>7 days</p></td>
<td class="text-center"><p>normal</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>accel</p></td>
<td><p>jobs needing GPUs</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>7 days</p></td>
<td class="text-center"><p>normal</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>devel</p></td>
<td><p>development jobs</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>2 hours</p></td>
<td class="text-center"><p>normal</p></td>
</tr>
</tbody>
</table>
<p>Note that job limits will change after pilot period is over.</p>
</section>
<section id="special-notes">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Special notes</a><a class="headerlink" href="#special-notes" title="Link to this heading"></a></h3>
<p>At this point running OpenMPI jobs that use a single compute node requires a special sbatch argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --network=single_node_vni</span>
</pre></div>
</div>
<p>OpenMPI applications can be started with <code class="docutils literal notranslate"><span class="pre">mpirun</span></code> (native OpenMPI way), or <code class="docutils literal notranslate"><span class="pre">srun</span></code>. In the latter case on Olivia this has to be done in the following way</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">srun</span> <span class="o">--</span><span class="n">mpi</span><span class="o">=</span><span class="n">pmix</span> <span class="o">&lt;...&gt;</span>
</pre></div>
</div>
<p>Applications compiled with Intel MPI and Cray MPI should not use this extra argument to <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
<section id="normal-jobs-cpus">
<h4>Normal jobs (CPUs)<a class="headerlink" href="#normal-jobs-cpus" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p><strong>Allocation units</strong>: CPUs and memory</p></li>
<li><p><strong>Job Limits</strong>:</p>
<ul>
<li><p>no limits</p></li>
</ul>
</li>
<li><p><strong>Maximum walltime</strong>: 7 days</p></li>
<li><p><strong>Priority</strong>: normal</p></li>
<li><p><strong>Available resources</strong>:</p>
<ul>
<li><p>252 nodes each with 2*128 CPUs and 768 GiB RAM</p></li>
</ul>
</li>
<li><p><strong>Parameter for sbatch/sallow</strong>:</p>
<ul>
<li><p>None, <em>normal</em> is the default</p></li>
</ul>
</li>
<li><p>Other notes:</p>
<ul>
<li><p>This is the default job type.</p></li>
<li><p>Normal jobs have $SCRATCH on local NVMe disk on the nodes</p></li>
</ul>
</li>
</ul>
<section id="normal-job-script">
<h5>Normal job script<a class="headerlink" href="#normal-job-script" title="Link to this heading"></a></h5>
<p>Job scripts for normal jobs on Olivia are very similar to the ones on
Saga.  For a job simple job running one process with 32 threads, the
following example is enough:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#SBATCH --account=nnXXXXk     # Your project
#SBATCH --job-name=MyJob      # Help you identify your jobs
#SBATCH --time=1-0:0:0        # Total maximum runtime. In this case 1 day
#SBATCH --cpus-per-task=32    # Number of CPU cores
#SBATCH --mem-per-cpu=3G      # Amount of CPU memory per CPU core
#SBATCH --network=single_node_vni # Currently required for OpenMPI to work with single compute node jobs.
</pre></div>
</div>
</section>
</section>
<section id="accel-jobs-gpu">
<h4>Accel jobs (GPU)<a class="headerlink" href="#accel-jobs-gpu" title="Link to this heading"></a></h4>
<p><em>accel</em> jobs give access to use the H200 GPUs.</p>
<ul class="simple">
<li><p><strong>Allocation units</strong>: CPUs, memory and GPUs</p></li>
<li><p><strong>Job Limits</strong>:</p>
<ul>
<li><p>no limits</p></li>
</ul>
</li>
<li><p><strong>Maximum walltime</strong>: 7 days</p></li>
<li><p><strong>Priority</strong>: normal</p></li>
<li><p><strong>Available resources</strong>: 76 nodes each with 4<em>72 ARM CPU cores, 4</em>120 GiB CPU RAM and 4*H200 GPUs (each with 96 GiB GPU memory).</p></li>
<li><p><strong>Parameter for sbatch/salloc</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">--partition=accel</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gpus=N</span></code>, <code class="docutils literal notranslate"><span class="pre">--gpus-per-node=N</span></code> or similar, with <em>N</em> being the number of GPUs</p></li>
</ul>
</li>
<li><p><strong>Other notes</strong>:</p>
<ul>
<li><p>Accel nodes have $SCRATCH on a shared flash storage file system.</p></li>
</ul>
</li>
</ul>
<section id="accel-job-script">
<h5>Accel job script<a class="headerlink" href="#accel-job-script" title="Link to this heading"></a></h5>
<p><em>Accel</em> jobs are specified just like <em>normal</em> jobs except that they also have
to specify <code class="docutils literal notranslate"><span class="pre">--partition=accel</span></code> and the number of GPUs to use.  You can also
specify how the GPUs should be distributed across nodes and tasks.  The
simplest way to do that is, with <code class="docutils literal notranslate"><span class="pre">--gpus=N</span></code> or <code class="docutils literal notranslate"><span class="pre">--gpus-per-node=N</span></code>, where <code class="docutils literal notranslate"><span class="pre">N</span></code>
is the number of GPUs to use.</p>
<p>For a job simple job running one process and using one H200 GPU, the
following example is enough:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>#SBATCH --account=nnXXXXk     # Your project
#SBATCH --job-name=MyJob      # Help you identify your jobs
#SBATCH --partition=accel
#SBATCH --gpus=1              # Total number of GPUs (incl. all memory of that GPU)
#SBATCH --time=1-0:0:0        # Total maximum runtime. In this case 1 day
#SBATCH --cpus-per-task=72    # All CPU cores of one Grace-Hopper card
#SBATCH --mem-per-gpu=100G    # Amount of CPU memory
#SBATCH --network=single_node_vni # Currently required for OpenMPI to work with single compute node jobs.
</pre></div>
</div>
<p>There are other GPU related specifications that can be used, and that
parallel some of the CPU related specifications.  The most useful are
probably:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--gpus-per-node</span></code> How many GPUs the job should have on each node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gpus-per-task</span></code> How many GPUs the job should have per task.
Requires the use of <code class="docutils literal notranslate"><span class="pre">--ntasks</span></code> or <code class="docutils literal notranslate"><span class="pre">--gpus</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--mem-per-gpu</span></code> How much (CPU) RAM the job should have for each GPU.
Can be used <em>instead of</em> <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu</span></code>, (but cannot be used
<em>together with</em> it).</p></li>
</ul>
<p>See <a class="reference external" href="https://slurm.schedmd.com/sbatch.html">sbatch</a> or <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">sbatch</span></code>
for the details, and other GPU related specifications.</p>
</section>
</section>
</section>
<section id="interactive-jobs">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Interactive jobs</a><a class="headerlink" href="#interactive-jobs" title="Link to this heading"></a></h3>
<p>For technical reasons, interactive jobs started with <code class="docutils literal notranslate"><span class="pre">salloc</span></code> will for
the time being <strong>not</strong> start a shell on the first allocated compute node
of the job.  Instead, a shell is started on the <em>login node</em> where you
ran <code class="docutils literal notranslate"><span class="pre">salloc</span></code>.  (This will be changed before the pilot period is over.)</p>
<p>This means that the commands are run on the login node, not in the
compute node.  To run a command on the compute nodes in the
interactive job, use <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">&lt;command&gt;</span></code> to run it there.</p>
<p>Alternatively, you can directly get a shell on the compute node with:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>srun<span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">1</span><span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">32</span><span class="w"> </span>--account<span class="o">=</span>nnXXXXk<span class="w"> </span>--time<span class="o">=</span><span class="m">1</span>:00:00<span class="w"> </span>--gpus<span class="o">=</span><span class="m">1</span><span class="w"> </span>--partition<span class="o">=</span>accel<span class="w"> </span>--pty<span class="w"> </span>bash
</pre></div>
</div>
<p>The trick is the <code class="docutils literal notranslate"><span class="pre">--pty</span></code> parameter combined with <code class="docutils literal notranslate"><span class="pre">bash</span></code> to get an interactive bash shell.</p>
</section>
</section>
<section id="internet-access-from-compute-nodes">
<span id="olivia-internet-proxies"></span><h2><a class="toc-backref" href="#id21" role="doc-backlink">Internet access from compute nodes</a><a class="headerlink" href="#internet-access-from-compute-nodes" title="Link to this heading"></a></h2>
<p>While the login nodes are directly connected to the public internet, the compute nodes are not.
To access external resources, for example software repositories, you can use proxies that we provide.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">http_proxy</span><span class="o">=</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">10.63.2.48</span><span class="p">:</span><span class="mi">3128</span><span class="o">/</span>
<span class="n">export</span> <span class="n">https_proxy</span><span class="o">=</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="mf">10.63.2.48</span><span class="p">:</span><span class="mi">3128</span><span class="o">/</span>
</pre></div>
</div>
<p>Please be aware that this works only for http/https resources.
For example to access a GitHub repo, you have to use HTTPS and not SSH.</p>
</section>
<section id="storage">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">Storage</a><a class="headerlink" href="#storage" title="Link to this heading"></a></h2>
<p>Home directories and project directories are found in the usual place:
<code class="docutils literal notranslate"><span class="pre">/cluster/home</span></code> and <code class="docutils literal notranslate"><span class="pre">/cluster/projects</span></code>.  These have disk quotas; see
<code class="docutils literal notranslate"><span class="pre">dusage</span></code> for usage.</p>
<p>On Olivia, there is no personal work areas
(<code class="docutils literal notranslate"><span class="pre">/cluster/work/users/&lt;uname&gt;</span></code>).  Instead, there are <em>project</em> work
areas, one for each project, in <code class="docutils literal notranslate"><span class="pre">/cluster/work/projects/</span></code>.  This makes
it easier to share temporary files within the projects.  There is no
disk quotas in the project work areas.  Later, old files will be
deleted automatically, just like in the user work areas on the other
clusters.</p>
<!--
## Debugging
## Data Storage/access(NIRD) and Transfer
## Guides to use Olivia effectively
### Best Practices on Olivia
## Any monitoring tools for users?
## User stories
## Troubleshooting
## Relevant tutorials
-->
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Sigma2/NRIS. Text shared under CC-BY 4.0 license.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>